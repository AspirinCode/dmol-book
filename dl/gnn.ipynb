{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Networks\n",
    "\n",
    "The biggest difficulty for deep learning with molecules is the choice and computation of \"descriptors\". Graph neural networks (GNNs) are a category of deep neural networks whose inputs are graphs. As usual, they are composed of specific layers that input a graph and those layers are what we're interested in. You can find reviews of GNNs in X, Y, and Z. Before we dive too deep into them, we must first understand how a graph is represented and how molecules are converted into graphs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing a Graph\n",
    "\n",
    "A graph $\\mathbf{G}$ is a set of nodes $\\mathbf{V}$ and edges $\\mathbf{E}$. In our setting, node $i$ is defined by a vector $\\vec{v}_i$, so that the set of nodes can be written as a rank 2 tensor. The edges can be represented as an adjacency matrix $\\mathbf{E}$, where if $e_{ij} = 1$ then nodes $i$ and $j$ are connected by an edge. In many fields, graphs are often immediately simplified to be directed and acyclic, which simplifies things. Molecules are instead undirected and have cycles (rings). Thus, our adjacency matrices are always symmetric $e_{ij} = e_{ji}$. Often our edges themselves have features, so that $e_{ij}$ is itself a vector. Then the adjacency matrix becomes a rank 3 tensor. Examples of edge features might be covalent bond order or distance between two nodes.\n",
    "\n",
    "\n",
    "Let's see how a graph can be constructed from a molecule. Consider methanol, shown in Figure. I've numbered the atoms so that we have an order for defining the nodes/edges. First, the node features. You can use anything for node features, but often we'll begin with one-hot encoded feature vectors:\n",
    "\n",
    "| Node | C  | H  | O  |\n",
    "|:-----|----|----|---:|\n",
    "| 1    | 0  | 1  |  0 |\n",
    "| 2    | 0  | 1  |  0 |\n",
    "| 3    | 0  | 1  |  0 |\n",
    "| 4    | 1  | 0  |  0 |\n",
    "| 5    | 0  | 0  |  1 |\n",
    "| 6    | 0  | 1  |  0 |\n",
    "\n",
    "$\\mathbf{V}$ will be the combined feature vectors of these nodes. The adjacency matrix $\\mathbf{E}$ will look like:\n",
    "\n",
    "\n",
    "|    | 1  | 2  | 3  | 4  | 5  | 6  | \n",
    "|:---|----|----|----|----|----|---:|\n",
    "| 1  | 0  | 0  | 0  | 1  | 0  |  0 |\n",
    "| 2  | 0  | 0  | 0  | 1  | 0  |  0 |\n",
    "| 3  | 0  | 0  | 0  | 1  | 0  |  0 |\n",
    "| 4  | 1  | 1  | 1  | 0  | 1  |  0 |\n",
    "| 5  | 0  | 0  | 0  | 1  | 0  |  1 |\n",
    "| 6  | 0  | 0  | 0  | 0  | 1  |  0 |\n",
    "\n",
    "\n",
    "Take a moment to understand these two. For example, notice that rows 1, 2, and 3 only have the 4th column as non-zero. That's because atoms 1-3 are bonded only to carbon (atom 4). Also, the diagonal is always 0 because atoms cannot be bonded with themselves. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import rdkit, rdkit.Chem, rdkit.Chem.rdDepictor, rdkit.Chem.Draw\n",
    "import networkx as nx\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_context('notebook')\n",
    "sns.set_style('dark',  {'xtick.bottom':True, 'ytick.left':True, 'xtick.color': '#666666', 'ytick.color': '#666666',\n",
    "                        'axes.edgecolor': '#666666', 'axes.linewidth':     0.8 , 'figure.dpi': 300})\n",
    "color_cycle = ['#1BBC9B', '#F06060', '#5C4B51', '#F3B562', '#6e5687']\n",
    "mpl.rcParams['axes.prop_cycle'] = mpl.cycler(color=color_cycle) \n",
    "soldata = pd.read_csv('https://dataverse.harvard.edu/api/access/datafile/3407241?format=original&gbrecs=true')\n",
    "np.random.seed(0)\n",
    "my_elements = {6: 'C', 8: 'O', 1: 'H'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiles2graph(sml):\n",
    "    '''Argument for the RD2NX function should be a valid SMILES sequence\n",
    "    returns: the graph\n",
    "    '''\n",
    "    m = rdkit.Chem.MolFromSmiles(sml)\n",
    "    m = rdkit.Chem.AddHs(m)\n",
    "    order_string = {rdkit.Chem.rdchem.BondType.SINGLE: 1,\n",
    "                    rdkit.Chem.rdchem.BondType.DOUBLE: 2,\n",
    "                    rdkit.Chem.rdchem.BondType.TRIPLE: 3,\n",
    "                    rdkit.Chem.rdchem.BondType.AROMATIC: 4}\n",
    "    N = len(list(m.GetAtoms()))\n",
    "    nodes = np.zeros((N,len(my_elements)))\n",
    "    lookup = list(my_elements.keys())\n",
    "    for i in m.GetAtoms():\n",
    "        nodes[i.GetIdx(), lookup.index(i.GetAtomicNum())] = 1\n",
    "    \n",
    "    adj = np.zeros((N,N,5))\n",
    "    for j in m.GetBonds():\n",
    "        u = min(j.GetBeginAtomIdx(),j.GetEndAtomIdx())\n",
    "        v = max(j.GetBeginAtomIdx(),j.GetEndAtomIdx())        \n",
    "        order = j.GetBondType()\n",
    "        if order in order_string:\n",
    "            order = order_string[order]\n",
    "        else:\n",
    "            raise Warning('Ignoring bond order' + order)\n",
    "        adj[u, v, order] = 1        \n",
    "        adj[v, u, order] = 1        \n",
    "    return nodes, adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes, adj = smiles2graph('CO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Graph Neural Network\n",
    "\n",
    "A graph neural network (GNN) is a neural network with two defining attributes:\n",
    "\n",
    "1. It's input is a graph\n",
    "2. It's output is permutation invariant\n",
    "\n",
    "We can understand clearly the first point. Here, a graph permutation means re-ordering our nodes. In our methanol example above, we could have easily made the carbon be atom 1 instead of atom 4. Our new adjacency matrix would then be:\n",
    "\n",
    "|    | 1  | 2  | 3  | 4  | 5  | 6  | \n",
    "|:---|----|----|----|----|----|---:|\n",
    "| 1  | 0  | 1  | 1  | 1  | 1  |  0 |\n",
    "| 2  | 1  | 0  | 0  | 0  | 0  |  0 |\n",
    "| 3  | 1  | 0  | 0  | 0  | 0  |  0 |\n",
    "| 4  | 1  | 0  | 0  | 0  | 1  |  0 |\n",
    "| 5  | 1  | 0  | 0  | 0  | 0  |  1 |\n",
    "| 6  | 0  | 0  | 0  | 0  | 1  |  0 |\n",
    "\n",
    "\n",
    "```{margin}\n",
    "Ok, so technically we might want our GNN to be permutation *equivariant*. If our GNN outputs per-node features, then obviously if we swap the node order of input, we want our per-node output to swap.\n",
    "```\n",
    "\n",
    "A GNN is permutation invariant if the output is insensitive to these kind of exchanges. Of course, there may exist GNNs out there which are not permutation invariant, especially if they are for trees where it is possible to deterministically order all nodes. Yet all the GNNs used in chemistry and most of the deep learning work is concerned with GNNs that are permutation invariant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple GNN\n",
    "\n",
    "We will often mention a GNN when we really mean a layer from a GNN. Most GNNs implement a specific layer that can deal with graphs, and so usually we are only concerned with this layer. Let's see an example of a simple layer for a GNN:\n",
    "\n",
    "\\begin{equation}\n",
    "f_k = \\sigma\\left( \\sum_i \\sum_jn_{ij}w_{jk}  \\right)\n",
    "\\end{equation}\n",
    "\n",
    "This equation shows that we first multiply every node feature by trainable weights $w_{jk}$, sum over all node features, and then apply an activation. This will yield a single feature vector for the graph. Is this equation permutation invariant? Yes, because the node index in our expression is index $i$ which can be re-ordered without affecting the output.\n",
    "\n",
    "Let's see an example that is similar, but not permutation invariant:\n",
    "\n",
    "\\begin{equation}\n",
    "f_k = \\sigma\\left( \\sum_i n_{ij}w_{ik}  \\right)\n",
    "\\end{equation}\n",
    "\n",
    "This is a small change. We have one weight vector per node now. This makes the trainable weights depend on the ordering of the nodes. Then if we swap the node ordering, our weights will no longer align. So if we were to input two methanol molecules, which should have the same output, but we switched two atom numbers, we would get different answers. These simple examples differ from real GNNs in two important ways: (i) they give a single feature vector output, which throws away per-node information, and (ii) they do not use the adjacency matrix. Let's see a real GNN that has these properties while maintaining permutation invariance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kipf & Welling GCN\n",
    "\n",
    "One of the first popular GNNs is the Kipf & Welling graph convolutional network (GCN) {cite}`kipf2016semi`. Although some people consider GCNs to be a broad class of GNNs, we'll use GCNs to refer specifically the Kipf & Welling GCN. \n",
    "Thomas Kipf has written an [excellent article introducing the GCN](https://tkipf.github.io/graph-convolutional-networks/). I will not repeat this article, so please take a look at it.\n",
    "\n",
    "The input to a GCN layer is $\\mathbf{V}$, $\\mathbf{E}$ and it outputs an updated $\\mathbf{V}'$. Each node feature vector is updated. The way it updates a node feature vector is by averaging the feature vectors of its neigbhors, as determined by $\\mathbf{E}$. The choice of averaging over neigbhors is what makes a GCN layer permutation invariant. Averaging over neighbors is not trainable, so we must add trainable parameters. We multiply the neighbor features by a trainable matrix before the averaging, which gives the GCN the ability to learn. In Einstein notation, this process is:\n",
    "\n",
    "\\begin{equation}\n",
    "v_{il} = \\sigma\\left(\\frac{1}{d_i}e_{ij}v_{jk}w_{lk}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "where $i$ is the node we're considering, $j$ is the neighbor index, $k$ is the node input feature, $l$ is the ouput node feature, $d_i$ is the degree of node i (which makes it an average instead of sum), $e_{ij}$ isolates neighbors so that all non-neighbor $v_{jk}$s are zero, $\\sigma$ is our activation, and $w_{lk}$ is the trainable weights. This equation is a mouthful, but it truly just is the average over neighbors with a trainable matrix thrown in. One common modification is to make all nodes neighbors of themselves. This is so that the output node features $v_{il}$ depends on the input features $v_{ik}$. We do not need to change our equation, just make the adjacency matrix have $1$s on the diagonal instead of $0$ by adding the identity matrix during pre-processing.\n",
    "\n",
    "Building understanding about the GCN is important for understanding other GNNs. You can view the GCN layer as a way to \"communicate\" between a node and its neigbhors. The output for node $i$ will depend only on its immediate neigbhors. For chemistry, this is not satisfactory. So you can stack multiple layers. If you have two layers, then the output for node $i$ will include information about node $i$'s neighbors' neigbhors. Another important detail to understand in GCNs is that the averaging procedure accomplishes two goals: (i) it gives permutation invariance by removing the effect of neighbor order and (ii) it prevents a change in magnitude in node features. A sum would accomplish (i) but would cause the magnitude of the node features to grow after each layer. Of course, you could ad-hoc put a batch normalization layer after each GCN layer to keep output magnitudes stable but averaging is easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/papermill.record/image/png": "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\n",
      "application/papermill.record/text/plain": "<Figure size 432x288 with 1 Axes>"
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "dframe"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from myst_nb import glue\n",
    "from moviepy.editor import VideoClip\n",
    "from moviepy.video.io.bindings import mplfig_to_npimage\n",
    "\n",
    "def draw_vector(x, y, s, v, ax, cmap, **kwargs):\n",
    "    x += s / 2\n",
    "    y += s /2 \n",
    "    for vi in v:        \n",
    "        if cmap is not None:\n",
    "            ax.add_patch(mpl.patches.Rectangle((x, y), s * 1.5, s, facecolor=cmap(vi),**kwargs))\n",
    "        else:\n",
    "            ax.add_patch(mpl.patches.Rectangle((x, y), s * 1.5, s, facecolor='#FFF', edgecolor='#333',**kwargs))\n",
    "        ax.text(x + s * 1.5 / 2, y + s/2, '{:.2f}'.format(vi), verticalalignment='center', horizontalalignment='center')\n",
    "        y += s        \n",
    "def draw_key(x, y, s, v, ax, cmap, **kwargs):    \n",
    "    x += s / 2    \n",
    "    y += s /2     \n",
    "    for vi in v:        \n",
    "        ax.add_patch(mpl.patches.Rectangle((x, y), s * 1.5, s, facecolor=cmap(1.),**kwargs))\n",
    "        ax.text(x + s * 1.5 / 2, y + s/2, vi, verticalalignment='center', horizontalalignment='center')\n",
    "        y += s\n",
    "    ax.text(x,y + s/2, 'Key:', verticalalignment='center', horizontalalignment='left')\n",
    "def draw(nodes, adj, ax, highlight=None, key=False, labels=None, mask=None, draw_nodes=None):    \n",
    "    G = nx.Graph()\n",
    "    for i in range(adj.shape[0]):\n",
    "        for j in range(adj.shape[0]):\n",
    "            if np.any(adj[i, j]):\n",
    "                G.add_edge(i, j)\n",
    "    if mask is None:\n",
    "        mask = [True] * len(G)\n",
    "    if draw_nodes is None:\n",
    "        draw_nodes = nodes\n",
    "    # go from atomic number to element\n",
    "    elements = np.argmax(draw_nodes, axis=-1)\n",
    "    el_labels = {i: list(my_elements.values())[e] for i,e in enumerate(elements)}\n",
    "    pos = nx.nx_agraph.graphviz_layout(G, prog='sfdp')\n",
    "    pos = nx.rescale_layout_dict(pos)\n",
    "    c = ['white'] * len(G)    \n",
    "    all_h = []\n",
    "    if highlight is not None:        \n",
    "        for i,h in enumerate(highlight):\n",
    "            for hj in h:\n",
    "                c[hj] = 'C{}'.format(i)                \n",
    "                all_h.append(hj)\n",
    "    nx.draw(G, ax=ax, pos=pos, labels=el_labels, node_size=700, node_color=c)\n",
    "    cmap = plt.get_cmap('Wistia')    \n",
    "    for i in range(len(G)):\n",
    "        if not mask[i]:\n",
    "            continue\n",
    "        if i in all_h:\n",
    "            draw_vector(*pos[i], 0.15, nodes[i], ax, cmap)\n",
    "        else:\n",
    "            draw_vector(*pos[i], 0.15, nodes[i], ax, None)\n",
    "    if key:\n",
    "        draw_key(-1, -1, 0.15, my_elements.values(), ax, cmap)\n",
    "    if labels is not None:\n",
    "        legend_elements = []\n",
    "        for i,l in enumerate(labels):\n",
    "            p = mpl.lines.Line2D([0], [0], marker='o', color='C{}'.format(i), label=l,\n",
    "                          markersize=15)\n",
    "            legend_elements.append(p)        \n",
    "        ax.legend(handles=legend_elements)\n",
    "    ax.set_xlim(-1.2, 1.2)\n",
    "    ax.set_ylim(-1.2, 1.2)\n",
    "    \n",
    "plt.figure()\n",
    "draw(nodes, adj, plt.gca(), highlight=[[1], [5, 0]], labels=['center', 'neighbors'])\n",
    "glue('dframe', plt.gcf(), display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] 0 5 0\n",
      "[False, False, False, False, False, True] 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:   0%|          | 0/48 [00:00<?, ?it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building file gcn.gif with imageio.\n",
      "[0] 0 5 0.0\n",
      "[False, False, False, False, False, True] 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:   4%|▍         | 2/48 [00:00<00:12,  3.79it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] 0 5 0.5\n",
      "[False, False, False, False, False, True] 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:   6%|▋         | 3/48 [00:01<00:15,  2.93it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] 0 5 1.0\n",
      "[False, False, False, False, False, True] 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:   8%|▊         | 4/48 [00:01<00:17,  2.52it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] 0 5 1.5\n",
      "[False, False, False, False, False, True] 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  10%|█         | 5/48 [00:02<00:18,  2.30it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] 0 1 2.0\n",
      "[False, True, False, False, False, True] 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  12%|█▎        | 6/48 [00:02<00:19,  2.16it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] 0 1 2.5\n",
      "[False, True, False, False, False, True] 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  15%|█▍        | 7/48 [00:03<00:19,  2.08it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] 0 1 3.0\n",
      "[False, True, False, False, False, True] 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  17%|█▋        | 8/48 [00:03<00:19,  2.03it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] 0 1 3.5\n",
      "[False, True, False, False, False, True] 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  19%|█▉        | 9/48 [00:04<00:19,  1.98it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] 0 0 4.0\n",
      "[True, True, False, False, False, True] 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  21%|██        | 10/48 [00:04<00:19,  1.95it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] 0 0 4.5\n",
      "[True, True, False, False, False, True] 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  23%|██▎       | 11/48 [00:05<00:19,  1.92it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] 0 0 5.0\n",
      "[True, True, False, False, False, True] 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  25%|██▌       | 12/48 [00:05<00:19,  1.88it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] 0 0 5.5\n",
      "[True, True, False, False, False, True] 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  27%|██▋       | 13/48 [00:06<00:18,  1.85it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] 0 2 6.0\n",
      "[True, True, True, False, False, True] 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  29%|██▉       | 14/48 [00:07<00:20,  1.63it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] 0 2 6.5\n",
      "[True, True, True, False, False, True] 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  31%|███▏      | 15/48 [00:07<00:19,  1.66it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] 0 2 7.0\n",
      "[True, True, True, False, False, True] 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  33%|███▎      | 16/48 [00:08<00:18,  1.70it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] 0 2 7.5\n",
      "[True, True, True, False, False, True] 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  35%|███▌      | 17/48 [00:08<00:17,  1.72it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] 0 3 8.0\n",
      "[True, True, True, True, False, True] 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  38%|███▊      | 18/48 [00:09<00:17,  1.73it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] 0 3 8.5\n",
      "[True, True, True, True, False, True] 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  40%|███▉      | 19/48 [00:10<00:16,  1.74it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] 0 3 9.0\n",
      "[True, True, True, True, False, True] 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  42%|████▏     | 20/48 [00:10<00:15,  1.77it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] 0 3 9.5\n",
      "[True, True, True, True, False, True] 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  44%|████▍     | 21/48 [00:11<00:15,  1.78it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] 0 4 10.0\n",
      "[True, True, True, True, True, True] 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  46%|████▌     | 22/48 [00:11<00:14,  1.79it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] 0 4 10.5\n",
      "[True, True, True, True, True, True] 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  48%|████▊     | 23/48 [00:12<00:13,  1.80it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] 0 4 11.0\n",
      "[True, True, True, True, True, True] 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  50%|█████     | 24/48 [00:12<00:13,  1.79it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] 0 4 11.5\n",
      "[True, True, True, True, True, True] 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  52%|█████▏    | 25/48 [00:13<00:14,  1.58it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recomputing\n",
      "[1] 1 5 0.0\n",
      "[False, False, False, False, False, True] 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  54%|█████▍    | 26/48 [00:14<00:13,  1.60it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1 5 0.5\n",
      "[False, False, False, False, False, True] 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  56%|█████▋    | 27/48 [00:14<00:12,  1.64it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1 5 1.0\n",
      "[False, False, False, False, False, True] 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  58%|█████▊    | 28/48 [00:15<00:12,  1.64it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1 5 1.5\n",
      "[False, False, False, False, False, True] 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  60%|██████    | 29/48 [00:15<00:11,  1.65it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1 1 2.0\n",
      "[False, True, False, False, False, True] 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  62%|██████▎   | 30/48 [00:16<00:10,  1.69it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1 1 2.5\n",
      "[False, True, False, False, False, True] 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  65%|██████▍   | 31/48 [00:17<00:09,  1.72it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1 1 3.0\n",
      "[False, True, False, False, False, True] 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  67%|██████▋   | 32/48 [00:17<00:09,  1.73it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1 1 3.5\n",
      "[False, True, False, False, False, True] 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  69%|██████▉   | 33/48 [00:18<00:08,  1.73it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1 0 4.0\n",
      "[True, True, False, False, False, True] 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  71%|███████   | 34/48 [00:18<00:08,  1.70it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1 0 4.5\n",
      "[True, True, False, False, False, True] 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  73%|███████▎  | 35/48 [00:19<00:07,  1.66it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1 0 5.0\n",
      "[True, True, False, False, False, True] 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  75%|███████▌  | 36/48 [00:20<00:07,  1.61it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1 0 5.5\n",
      "[True, True, False, False, False, True] 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  77%|███████▋  | 37/48 [00:20<00:06,  1.62it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1 2 6.0\n",
      "[True, True, True, False, False, True] 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  79%|███████▉  | 38/48 [00:21<00:06,  1.66it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1 2 6.5\n",
      "[True, True, True, False, False, True] 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  81%|████████▏ | 39/48 [00:22<00:06,  1.46it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1 2 7.0\n",
      "[True, True, True, False, False, True] 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  83%|████████▎ | 40/48 [00:22<00:05,  1.50it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1 2 7.5\n",
      "[True, True, True, False, False, True] 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  85%|████████▌ | 41/48 [00:23<00:04,  1.56it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1 3 8.0\n",
      "[True, True, True, True, False, True] 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  88%|████████▊ | 42/48 [00:23<00:03,  1.61it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1 3 8.5\n",
      "[True, True, True, True, False, True] 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  90%|████████▉ | 43/48 [00:24<00:03,  1.64it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1 3 9.0\n",
      "[True, True, True, True, False, True] 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  92%|█████████▏| 44/48 [00:25<00:02,  1.67it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1 3 9.5\n",
      "[True, True, True, True, False, True] 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  94%|█████████▍| 45/48 [00:25<00:01,  1.68it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1 4 10.0\n",
      "[True, True, True, True, True, True] 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  96%|█████████▌| 46/48 [00:26<00:01,  1.68it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1 4 10.5\n",
      "[True, True, True, True, True, True] 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:  98%|█████████▊| 47/48 [00:26<00:00,  1.70it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1 4 11.0\n",
      "[True, True, True, True, True, True] 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t: 100%|██████████| 48/48 [00:27<00:00,  1.69it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1 4 11.5\n",
      "[True, True, True, True, True, True] 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqsAAAD3CAYAAAA6yB1UAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABqz0lEQVR4nO3deVzU1f748dfMMAMMiwu4b6ggpImASLikV7xmlrhwb2b6VdNSU0tLy7pZiWbdutpyNbpmZV71dlt+JahlbpmV5m6aBSgqbpg7KDMw6+f3B9fRCVSWYWDg/Xw8eDzy8zlz5nxOzJv3nM/5nKNSFEVBCCGEEEKIakhd1Q0QQgghhBDiZiRZFUIIIYQQ1ZYkq0IIIYQQotqSZFUIIYQQQlRbkqwKIYQQQohqS5JVIYQQQghRbUmyKoQQQgghqi1JVmu4hIQEtm3bVtXNcHLu3Dkee+wxevToQXh4OKdOnbpleXddw8KFC3n66acr/X2EEAJqRnwG+PLLL0lMTKRTp050796dWbNmceXKlVK/p6v7oTr2q6gYSVZFpbJarcWOqdVq7r77bhYuXFgFLRJCCAGuic9Llixh/vz5PPPMM+zevZtPP/2UnJwcxowZg9lsdnWTRS0lyWotlZeXx4QJE4iPj6dLly5MmDCB33//HYC1a9eSlJTkVP6jjz5i4sSJAJjNZl5//XX+9Kc/0a1bN1566SUKCwsB2LFjBz179mTx4sV0796dv/3tb8XeOzg4mBEjRtCxY8cyt/vLL7/koYce4vXXX6dLly4kJCSwZcsWx/mRI0fyxhtv8Ne//pWYmBgmTpxIbm6uU9tudO0b+Pfff897773H2rVriY6OZuDAgWVumxBCuIKnxOf8/HwWLlzICy+8QM+ePdFqtTRv3py3336b06dPs2rVKgCee+453nrrLcfrbozFzzzzDDk5OTz22GNER0fz/vvvc+rUKcLDw/n000/p0aMHPXr04MMPP3S8vqz1Cc8nyWotZbfbSUpKYvPmzWzevBlvb2/mzJkDQJ8+fTh16hRHjhxxlE9LS2Pw4MEAzJ8/n2PHjpGamsr69es5d+4cKSkpjrIXLlwgLy+PzZs38/LLL7u87QcOHKB169Zs376dRx99lJkzZ3LjrsGpqam8+uqr/Pjjj3h5eTF37tzb1tmzZ08mTJhA//792bdvnyPICiGEu3lKfN67dy8mk4l77rnH6bifnx+9evUq1a34efPm0bRpUxYtWsS+ffsYN26c49yOHTtYv349H374Ie+//36F6xOeS5LVWqpevXr069cPX19f/P39mThxIrt27QJAp9PRv39/R8J2+PBhTp8+Te/evVEUhc8++4znn3+eunXr4u/vz4QJE/jqq68cdavVaqZMmYJOp8PHx8flbW/atClDhw5Fo9EwZMgQzp8/z4ULFxznBw0aRLt27dDr9UydOpVvvvkGm83m8nYIIURl8JT4fPnyZerVq4eXl1excw0aNODy5csVqn/y5Mno9XrCw8NJSkpizZo1FapPeK7iv2GiVigoKODvf/87P/zwA3l5eQAYDAZsNpsjCZw2bRpPPvkkaWlp9O/fH51Ox8WLFykoKHC6DaUoCna73fHvevXq4e3tXWltDw4Odvy3r68vAEaj0XGsSZMmjv9u2rQpFoulwkFTCCHcxVPic7169bh8+TJWq7VYwnr+/Hnq1atXofpvjOXNmjXj0KFDFapPeC5JVmupJUuWcOzYMT777DMaNGhAeno6gwcPdtxOj4qKQqvVsnv3btasWcP8+fOBouDk4+PDV199RaNGjUqsW6VSue06SnLmzBmn/9ZqtdSrVw9fX1/H3C0Am83GpUuXHP+u6nYLIQR4TnyOjo5Gp9Oxfv167rvvPsdxg8HA999/z7Rp0wCKxd4b74TdypkzZ2jbti0AOTk5NGzYsEL1Cc8l0wBqAYvFgslkcvxYrVYMBgPe3t4EBgaSm5vLO++8U+x1gwcPZs6cOXh5eREbGwsU3UJ64IEHePXVV7l48SIAZ8+e5YcffihTm0wmk+NJUbPZjMlkquBVXrdq1SqysrIoKCjgn//8J/369UOj0dC6dWtMJhPfffcdFouFf/3rX05PqwYFBXH69GmnUQghhKhMnhyfAwICmDx5MnPnzuX777/HYrFw6tQpnnzySRo3bsygQYMAuOOOO9iyZQu5ubmcP3+ef//73071BAcHc/LkyWL1v/vuuxQUFHD48GG+/PJLR0Jc3vqE55JktRYYP348kZGRjp+FCxcyevRoTCYT8fHxPPjgg9x9993FXjdo0CAOHz5c7Mn4Z555hlatWjF06FBiYmJ4+OGHOXbsWJnaFBkZSXR0NAD9+/cnMjKy/BdYQrufe+45unfvjtlsZubMmUBRYJ01a5bjyVVfX18aN27seN29994LwF133cWQIUNc1h4hhLgZT4/P48aN46mnnuIf//gHnTt3ZujQoTRp0oSlS5ei0+kcbY2IiCAhIYGxY8c6jcJe64N//etfxMbGOj31HxcXR9++fXn44YcZO3YsPXr0qFB9wnOplBsfoxbiBoWFhXTt2pWVK1cSEhJS1c0plZEjRzJw4EAeeOCBqm6KEEJUGk+Mz6V16tQp+vTpw6+//lriw1ui9pGRVXFT//3vf+nYsWONC4RCCOHpJD6L2kS+sogSJSQkoCiK0/p8Qgghqp7EZ1HbyDQAIYQQQghRbck0ACGEEEIIUW1JsiqEEEIIIaotSVaFEEIIIUS1JcmqEEIIIYSotiRZFUIIIYQQ1ZYkq0IIIYQQotqSZFUIIYQQQlRb5doUwGKxcPLkSQoKCktVXlEUfrNcJdOST4Fiw1elIVzrT3ttACqVqjxNqHV8fX1o0aIFWq22qpsihBBCCOE25doU4OjRo3h5eePvX+eWyaZFsbPywhE+OpvORWshVkXBqtjxUqnxUqkI8vJhTKM7GBLcFq1KBnlvRlEUrl7Nw2Yz0aZNm6pujhBCCCGE25QrWf31199o0qTlLRNVo83CxKwtpBdcosBuu2k5H5WG9vr6/Cu0F3qNjBrejKIonDlzgg4d2ld1U4QQQggh3KZc0wCA246oTszawkHjRcyK/Zb1FCo2DhovMjFrCx+0S5AR1puQ6RJVIzExkTNnzrj1PZs0acLq1avd+p5CCFETSMyumcqdrN7KygtHSC+4dNtE9RqzYuc34yVSLxzhgQZhldEkIcrlzJkz7N69263vGRsb69b3E0KImkJids3k8mFMRVH46Gz6LW/9l6RQsbHkbDrlmJXgcu+/vwiLxVLVzRDVTG5uLpMnTyYqKorevXvf9Jv0woUL6dChA9HR0Y6fkydPOs6np6eTlJREp06dSEpKIj093V2XIIQQtYarYnZ4eDhRUVGOczNnznTXJYj/cfnI6n7DBS5aS7dKwB9dtJrYb7hAlH8DF7eqbD78cDEjRowq85P3VqsVL69KGawW1cCcOXPQarVs3bqV9PR0JkyYQEREBGFhxe8G9O/fn/nz5xc7bjabmTRpEqNHj2b48OF88sknTJo0iXXr1qHT6dxxGUIIUSu4ImZfk5aWRqtWrSqzueIWKpxZrbp4jJUXjzr+fdZspLCMo6rXFNqtPJ+9nUY6PQBDgtowMKh1qV77yy/7WbjwnxiNBgCeeOJJGjduwltvzScvLxeLxcKwYcMZMGAQAPHxMTz22GS2bNlMXl4ejz/+JAkJfZg37+8AjBv3MGq1mnfffR+1WsXbb7/JkSOHMZlMdO7chalTp6HRaJg4cRzt2rXj4MFfCAysw1tvLSzXtYvqzWg0sn79elavXo2fnx+xsbEkJCSQlpbG008/Xep6du7cidVqZfTo0ahUKkaNGsWSJUvYvn07PXv2rMQrEEKI2sNVMVtUDy4fBrShUN4b+cr/Xl9WeXl5PPvs07z22nwiIzths9m4cuUKTz45mdmzXyEkpDUGg4ExY/6PO++MJCSkKAH28/Pjo49WsH//z7zwwrMkJPThmWf+xhdffM777y9Fry9Kml95ZQ4xMTHMnPkSdrudWbNmsnp1GoMHJwFw+vRp3ntviYyq1mDZ2dloNBpat77+5SkiIoJdu3aVWH7z5s3ExcXRoEEDRowYwfDhwwHIysoiPDzc6YG58PBwsrKyJFkVQggXcVXMvmbEiBEoikJ0dDTPPfcczZs3r9T2C2cVzq4GBrV2Gv1cfi6Dt07vx1LKh6tupFWpGdkwnP9rGF6m1x08eIDWrVsTGdkJAI1GQ27uZbKzs3nxxb85ypnNZrKzjzmS1b59+wFw550dOX/+PCaTCW9v72L1//jjFn777SAff7wCgMLCQho2bOg4369ff0lUazij0Yi/v7/TsYCAAAwGQ7Gy/fv3Z+jQoQQHB7N//36mTJlCYGAgAwYMwGAwEBAQ4FTe39+/xHqEEEKUj6tiNsCKFSvo1KkThYWFvP322zz22GOkpqbK3303cnlPd9QH4aVSYSnH8KqXSs2d+vouaYeiKNStW5flyz+5aRmdrigx1Wg0ANhsJU9fUBSFf/zjTZo1K/mblK+vbwVbK6o7vV5Pfn6+07H8/Hz8/PyKlQ0NDXX8d0xMDKNGjWLdunUMGDAAPz+/YvUYDIYS6xFCCFE+rorZAF26dAFAp9Mxc+ZMOnfuzJEjRwgPL9vAmig/l68G0MkvmCAvn3K9NsjLm05+wWV+3Z13RnLs2DF++WU/UJR01qtXHx8fH9auXeMol519DIMh/2bVOOj1zgnF3Xf3YtmyjxzJbG7uZXJyTpe5ncJzhYSEYLPZyM7OdhzLyMhwCnK3cm2Vi9DQUDIzM51WvcjMzCx1PUIIIW7PVTG7JCqVqlqsXFSbuDxZValUjGl0B75qTZle56PSMLbRHeVa/L5OnTq89tp8/vnPNxkxYigPPzyCw4cPMW/e22zYsJ4RI4by0EN/Zd68v5dqSarhw/+Pxx+fwMiRw7h69SpPPvk0arWGkSOHMWLEUJ588nHOnTtX5nYKz6XX6+nbty8LFizAaDSyZ88eNm3axKBBg4qV3bhxI3l5eSiKwoEDB1i+fDl9+vQBIC4uDo1Gw7JlyzCbzaxYUTS1JD4+3q3XI4QQNZmrYvbhw4dJT0/HZrNhMBh47bXXaNiwIW3btnX3JdVq5d5utWnTmy/hYFHsPHro21LtYAWgU6npqA/ifdnB6pZyco7LdqtuFhsb61hgOjc3l+eff55t27ZRt25dpk+fTmJiIrt372bcuHHs27cPgGnTprF161bMZjONGjVi+PDhjBo1ylHnb7/9xgsvvEBWVhZt27bllVdeoX379iW+pxBCiNJzdcz+6aefSE5O5uzZs/j6+hIdHc2MGTMICQkp8T1F5aiUZBXAaLMwMWsLvxkvUajcfCkrH5WGDvr6vBvaC72mbOua1jaSrLpfVQQhCXxCCFE+ErNrpkp7lE2v0fJBuwRSLxxhydl0LlpNWBU7VsWOl0qNl0pNkJc3YxvdweDgtjKiKoQQQgghiqnUdRe0KjUPNAjjr0Ftycr4mSvHDqMUmlH5+BDYui2hEVGo1JKkCiGEEEKIklXuImE2G+zYhvrbjbTLv1r0b5sNNBr4YQf4p6Ek/Bnu6lZ0TIhqxsfHh9jYWLe/pxBCiLKTmF0zVV6yajKhWvwunD6Jymx2Pnctab10EdJWwp7dKOMnQQkL8gtRlQoLC6tk/pMQQoiyk5hdM1XOPXibrShRPXm8eKL6ByqLuajc4neLElghqqnc3FwmT55MVFQUvXv3ZvXq1SWWUxSFefPmcdddd3HXXXcxb948pzX50tPTSUpKolOnTiQlJZGenu6uSxBCiFqjtDF7+/btjBw5ks6dO5OQkFDs/KlTpxg5ciSdOnXi3nvvZdu2bZXddPEHlZOs7thWNKJqtZaquMpqhVMnYcdPldIcIVxhzpw5aLVatm7dyrx580hOTubw4cPFyn366ads3LiRtLQ0Vq1axebNm/nkk6Kd1MxmM5MmTWLgwIHs2rWLwYMHM2nSJMy3+VInhBCibEobs/V6PX/5y1+YMWNGifVMnz6d9u3bs2PHDp566immTJnCpUuXKrv54gauT1YVBdW3G287ovpHKosZ1bcbwE27Qpw/f55Jk8aXqmx8fAxGo7HM50TNYTQaWb9+PVOnTsXPz4/Y2FgSEhJIS0srVjY1NZWxY8fSuHFjGjVqxJgxY1i5ciUAO3fuxGq1Mnr0aHQ6HaNGjUJRFLZv3+7uSxJCiBqrLDE7MjKSwYMH06JFi2Lnjh07xq+//soTTzyBj48P/fr1o127dqxbt84dlyH+x/XJavYxyL9avtfmXy16vRs0aNCAd99d7Jb3uh2bTH+o9rKzs9FoNLRu3dpxLCIigqysrGJlDx8+TEREhFO5a9/ms7KyCA8Pd9qpLTw8vMR6hBBClE9ZYvatZGVl0aJFC/z9/StUj6iYij9gtWsHqp033L7PzYXy3tI0m1F9vAzq1gVAiesKXe667cvi42N47LHJbNmymby8PB5//EkSEoq2Sjt48BfefXchBkM+AOPHT6R797vJyclhzJj/Y926bwH49ttNvPdeCt7e3iQk/JlFi1L49tsf0ev1AHz22X9LrB/gP//5N99/vwWTycRjjz3uOPfTT1v517/ewWazUa9ePZ59diYtWrRkz57dvPnmP4iIuINDhzKZMGESFy6c57///Q86nQ673c4rr7xOSEhrRPVgNBqdghVAQEAABoPhtmUDAgIwGo0oioLBYCAgIMCpvL+/f4n1CCGEKJ+yxOxbKSlmBwQEcPbs2Qq3UZSe61cDsN9+e9VbKsX2rCXx8/Pjo49WsH//z7zwwrMkJPTh6tWr/OMfr/LmmwsIDm7AhQvnGTNmJB9//LnTay9evMhrr83lgw/+TcuWLfnvf1eUqv5r1GoNy5d/wvHj2YwbN4aoqGgAZs9+kX/96wNat27DqlWpzJr1AkuWLAPg2LGjPPfcTDp27ARAnz49+fTTLwgOboDZbMZul9HW6kSv15Ofn+90LD8/Hz8/vxLL3hgQ8/Pz0ev1qFQq/Pz8itVjMBhKrEcIIUT5lCVm30pJMbs89YiKqfg0gC53oUx+8vpPz97lXzNV44XSs7ejrtKMql7Tt28/AO68syPnz5/HZDLxyy/7yck5zVNPPcHIkcN46qknUKlUnDp10um1v/56kPDwCFq2bAlAYuKgUtV/TWLiYABatQohPDyCgwd/4ddffyE0tB2tW7cBYMCAgRw+nOlIYlq0aOlIVAFiY7swZ84sPvvsE86fP4ePj2+pr11UvpCQEGw2G9nZ2Y5jGRkZhIaGFisbFhZGRkaGU7mwsDAAQkNDyczMdFodIDMzs8R6hBBClE9ZYvathIaGcvLkSaeEtTz1iIpx/ZzVlq0qkKyqoUWrcr1Upytao1Xzv/e22WwoikJoaBjLl3/i+Fm1ai133NHeJfVXhK+vczL62mvzmTBhEoWFBUyePJ5t27ZWqH7hWnq9nr59+7JgwQKMRiN79uxh06ZNDBpU/IvNoEGD+Oijjzh79ixnz57lo48+YsiQIQDExcWh0WhYtmwZZrOZFSuKRvHj4+Pdej1CCFGTlSVm2+12TCYTFosFRVEwmUyOFVpat27NHXfcQUpKCiaTiQ0bNpCZmUm/fv3cfUm1muuT1ZDW4B9w+3IlCQgoer2LdOzYiZMnT7Jnzy7Hsd9++9VpVAugQ4c7yczMcIy4fvXVmjK9z5o1qwA4ceIEhw5lcuedHbnzzkiysg6R/b8Hxr7+ejXt2oWXeOvAarVy+vQpOnS4k1GjxhAX15VDhzKKlRNVa9asWRQWFtKtWzemT59OcnIyYWFh7N69m+joaEe5YcOG0bt3bxITE0lMTKRXr14MGzYMAJ1OR0pKCmlpacTGxvLFF1+QkpKCTqerqssSQogaqbQxe9euXURGRjJ+/HhycnKIjIzkkUcecZx/8803OXjwIF26dGH+/PksWLCA+vXrV8Ul1Vqun7OqUhVtobpqZZmWr1K0OpTefeGGp6QrKjAwkHnz3mLhwrd56635WCwWmjVrzvz5bzuVCwoK4tlnn2fatCn4+PjQvfvdeHl5lXoLNZvNyqhRD1FYWMizz850/BLPmvUyL700E5vNSr169UhOnlvi6+12Oy+/PIv8/HxUKhWNGjVi8uQnKnTtwvXq1q3Lu+++W+x4bGws+/btc/xbpVIxY8aMm67Z1759e7788stKa6cQQojSx+y77rqLzMzMm9bTvHlzli9fXiltFKWjUv44zFgKv/76G02b3uJ2vc2G6t0FRTtTlWJjAMXLC1q2Qpk4pfxTCCroxodc1qxJY9WqNBYvXlIlbbmZnJzjdOhQ9ikMovxiY2OrZOs+d7+nEELUBBKzaybXj6wCaDQo4yehWvwuyqmTRVuq3oSi1UGLFijjJlVZogpFS1N9++1GbDYbgYGB/O1vL1RZW0T14ePj4/Z9n0s7oi+EEMKZxOyaqXKSVQBvb5RJU4q2UP12Q9GC/zY72Kyg8Sp6mCogoOjW/11dqzRRBRgz5lHGjHm0Stsgqp/CwsIq+ZYuhBCi7CRm10zlTlYVRXHahadEGg1064HStXvRzlQnj4PJBN7eRasGtGrt0jmqNVk5ZmsIF8vNzWXmzJls3bqVevXqMW3aNBITE4uVW7hwIYsWLXJ6aGrVqlWOrfzS09OZOXMmR44coW3btrzyyivccccdbrsOIYSoDUobs5cuXcry5cu5fPkyer2e++67jxkzZuDlVZQiJSQkcOHCBcdqQNHR0SxZUr2mCdZ05UpWfX19uHo1j4CAOrdPWKEoIW3dpuhHlJmiKFy9moevr9xqqEpz5sxBq9WydetW0tPTmTBhAhEREY41VG/Uv39/5s+fX+y42Wxm0qRJjB49muHDh/PJJ58wadIk1q1bJysCCCGEC5U2ZickJJCUlERgYCC5ublMmTKF5cuXM2bMGEeZRYsW0a1bN3dfgvifciWrLVq04OTJk5w5c8LV7RE34evr4xiZE+5nNBpZv349q1evxs/Pj9jYWBISEkhLS+Ppp58udT07d+7EarUyevRoVCoVo0aNYsmSJWzfvp2ePXtW4hUIIUTtUZaYfW1DICgaHFKr1Rw/ftzdTRa3UK5kVavV0qaNjJKK2iM7OxuNRkPr1tfXAY6IiGDXrl0llt+8eTNxcXE0aNCAESNGMHz4cACysrIIDw93uiMRHh5OVlaWJKtCCOEiZY3Zq1evZtasWRgMBurVq8dzzz3ndP7pp5/GbrfTvn17ZsyYQURERKW2XzirvAeshKhBjEYj/v7+TscCAgIc2+feqH///gwdOpTg4GD279/PlClTCAwMZMCAARgMBgICnDfN8Pf3L7EeIYQQ5VOWmA04NnHJzs4mNTWVoKAgx7l58+bRoUMHFEVh2bJlPPLII6xdu5bAwMBKvQZxnet3sBKiBtLr9U57QwPk5+eXuCNZaGgojRo1QqPREBMTw6hRo1i3bh0Afn5+xeq5cY1fIYQQFVeWmH2jkJAQwsLCmD17tuNY586d8fHxwdfXlwkTJhAQECDrqrqZJKtClEJISAg2m43s7GzHsYyMDEJDQ0v1+murOYSGhpKZmem0ukNmZmap6xFCCHF7FYnZVquVEydu/kyOSqWSFXrcTJJVIUpBr9fTt29fFixYgNFoZM+ePWzatIlBgwYVK7tx40by8vJQFIUDBw6wfPly+vTpA0BcXBwajYZly5ZhNptZsWIFAPHx8W69HiGEqMnKErM///xzLl68CBQ9V7B48WK6du0KQE5ODnv27MFsNmMymfjggw+4fPkyMTExbr2e2k6SVSFu4cY5SbNmzaKwsJBu3boxffp0kpOTCQsLY/fu3URHRzvKff3119xzzz3ExMQwY8YMxo0bx5AhQwDQ6XSkpKSQlpZGbGwsX3zxBSkpKbJslRBCuMCNy1KVNmbv3buXxMREoqKiGD9+PD179mTatGlA0TSt5ORk4uLi6NmzJz/88APvv/8+9erVc/u11WYqRcayhbipawv3u5PsMy2EEOVz9erVYg+xVjaJ2ZVPRlaFuIVmzZpVdROEEEKUkjysWjNJsirELfj4yK5hQgjhKdRqSWtqIllnVYhSyMnJYeDAgW55ryZNmrjlfYQQoqYymUwkJiZy6dKlSn8vidmVT+asCiGEEEKIakvGy0WtERISwsaNG52OLV26lB49elRRi4QQQtyMxGxxjSSrQgghhBCi2pJkVQghhBBCVFuSrAohhBBCiGpLVgMQtcrgwYPx8rr+a282m2XbPCGEqKYkZguQkVVRy6SmppKbm+v4effdd6u6SUIIIW5CYrYASVaFEEIIIUQ1JsmqEEIIIYSotiRZFUIIIYQQ1ZbsYCWEEEIIIaotj1kNIDExkTNnzrj1PZs0acLq1avd+p5lIX3ioWyFoPGp6laUj7vb7sl9VctJfCqZu/vFE/qkMphMJh544AFycnIqXJdH92ENidkek6yeOXOG3bt3u/U9Y2Nj3fp+ZSV94ibWfNg/Hk7/1zX1DfTgmxkaH/hW5b73S/DgvqrlJD6VzN394gl9Uhm8vb1ZtWqVS+ry6D7U+MA3bozZ91ZOzPbIOau5ublMnjyZqKgoevfufdNvPAsXLqRDhw5ER0c7fk6ePOk4n56eTlJSEp06dSIpKYn09HR3XYLLlbZPli5dSp8+fYiJiaFHjx68+uqrWK1Wx/mRI0cSHx9PTEwMAwcOLLYvc01WYh/azXBpa7FE9ddT3ox4tznRM0PpNrsN//6hruPcqUtejFzUnE7Ph3LvP0LYdkjv5itxj1yDmsmLmhI1NZTeM1uzemdAieUWrgmiw+Qwop8MdfycPK91nE8/6U3Sqy3pNCWUpFdbkn7S212XINyktPHpGrPZTP/+/enZs6fj2LFjx5g4cSLx8fHExcXxyCOPcPTo0cpueqVxRZ8A/PTTTwwZMoSYmBj69OnDp59+WpnNrlZc1YfffvstAwYMIDo6mmHDhpGVlVWZza4yuQY1kz9oStQzofRObs3q3TeJ2WuD6PBUGNHPhDp+Tl64HrPDp7Yj6oZzM//bqNLb7jEjqzeaM2cOWq2WrVu3kp6ezoQJE4iIiCAsLKxY2f79+zN//vxix81mM5MmTWL06NEMHz6cTz75hEmTJrFu3Tp0Op07LsOlStsnCQkJJCUlERgYSG5uLlOmTGH58uWMGTMGgJkzZxIaGoqXlxf79+/n4YcfZt26dTRs2LAqLsutSuzD1vUJOz3Kqdwlg5pHP2jG3wae597I05itcDbv+gd5+n+aENWqkPcfOc2WdD+mLG/C+mezqe9vc/clVao5nzRE66Ww9fUjpJ/yZkJKMyKamwhrai5Wtn/sVeaP+b3YcbMVJi1qyuiEywzvmccnP9Rh0qKmrJt9DJ1HRidRkrLEbIAPP/yQ+vXrYzAYHMeuXr1KQkICf//73/Hz8yMlJYVJkybxzTffuOsyXMoVfWKxWHj88cd55plnePDBB/nll18YPXo0nTp1IiIiwl2XUmVc0YfZ2dk8/fTTLF68mKioKD788EMmTpzI2rVrnTYjqAnm/L//xey5/4vZi5sR0cxEWJMSYnb0VeaPKh6zr0mbcZxWDSyV2VwnHjeyajQaWb9+PVOnTsXPz4/Y2FgSEhJIS0srUz07d+7EarUyevRodDodo0aNQlEUtm/fXkktrzxl6ZOWLVsSGBgIgKIoqNVqjh8/7jgfERHh+ICqVCqsViu//37zX9iaolgfxkSS0KsraYtGgemcU9ml39ejR7iRgTFX0Xkp+PsotG1U9GE/dl7Lr6e9eeKeC/hoFfpF5tOuiYl1v/hXxWVVGqNJxfp9AUxNvICfj0JsaCEJkQbSdgSWqZ6dh/RYbSpGJ+Si0yqMSshFUWB7Zs0cja6NyhqzT548yapVqxg/frzT8cjISB544AHq1q2LVqvl4Ycf5tixY1y+fNkdl+FSruqTvLw88vPzGTRoECqVisjISNq0aVNjRwZv5Ko+/PHHH4mNjSU2NhYvLy/GjRvH2bNn2bVrlzsuw22MJhXr9wcw9b4L+HkrxLYtJOFOA2m7yhazq4rHJavZ2dloNBpat27tOBYREXHTD+fmzZuJi4vj/vvv5+OPP3Ycz8rKIjw8HJXq+lyO8PBwj/yQl7VPVq9eTUxMDPHx8WRkZDBs2DCn8xMmTKBjx4488MADxMXFceedd1Zq+6sDpz605sPFH4jwWkXWqcJiZX8+7ksdvY1h77Sga3IbHlvSlJzLRQl+1u/etAiy4O9zfd5ORBMzWWc9b7T+VrLP6dCoFVo3uv7NOqK5iawzJV/n5gN+xE1vy/1zWvHxljqO41lndIQ3M3HDx5DwZiayzshUgJqirPFp7ty5TJs2DR+fWz+ksXv3bho0aEC9evVc2l53cFWfBAcHM2DAAL788ktsNhv79u0jJyeHzp07V2r7qwNX/l7duCiSoigoisKhQ4dc3+gqlH3+fzG74Q0xu5mJrN9vErN/9SPub225/++t+PjHOsXOj1jQgu4vtOHxD5tw6mLlj0B73Bi30WjE3995lCogIMBpWP+a/v37M3ToUIKDg9m/fz9TpkwhMDCQAQMGYDAYCAhwnq/h7+9fYj3VXVn6BIqeRk1MTCQ7O5vU1FSCgoKczr/33ntYLBa2bdvG0aNHUas97jtNmRmNRvz99GC+CAefhFMrCNDWwWAqPqfnbJ4Xv532Zsn4U4Q3NjPvq2Cm/acJnzx+EoNZRYCP3al8gI+Ns3ke91G7JWOhGn/fP1ynrx1DYfHflf6drzK0Ry7BgTb2H/NhyuKmBOrtDOhyFYNJTYCv8/QI/5vUIzxTWeLThg0bsNls9O3blx07dty0zt9//53Zs2fz3HPPuby97uDKPrn//vt54YUXeOWVVwBITk6mSZMmldPwasRVfdi1a1fmz5/Pjh07iI6O5v3338disVBYWHygwpMZTWr8i/1tsmMwlRCzo68ytFsuwQE29h/3YcqSpgT62hnQ+SoAK544SaeQAgrNat7+OpjHFjcjdcZxvDSV136P+4ug1+vJz893Opafn4+fn1+xsqGhoTRq1AiNRkNMTAyjRo1i3bp1APj5+RWrx2AwlFhPdVeWPrlRSEgIYWFhzJ49u9g5rVZLr169+PHHH9m0aZNL21tVFEWhoKAAu935A4s1H705nfyrl2FdIzi1AoD8QjV+3vZi9XhrFfremU9kCxPeWoXJfS+y77gvVwvU+OkU8v+QaOWb1Pj5FK/Hk+l97OQX/OE6C0u+ztAmZhrVtaFRQ0zbQkYlXGbd3qI/Mn7edvILnSOc4Sb1CM9U2vhkNBqZN28eL7zwwi3ru3TpEmPHjmX48OEMGDDA5e11B1f1yZEjR5g2bRqvv/46Bw8eZM2aNXzwwQd89913ldV0t7NYLFgs10cDryWjrurDtm3b8tprr/Hyyy9z9913c/nyZUfuUJPove3F/zbd5G9caGMzjer8L2a3LmRUz8us+/n6F4MuoQXovCBQb2dm0jlOXdJypJLvHnpcshoSEoLNZiM7O9txLCMjg9DQ0FK9/tpwf2hoKJmZmU7D/5mZmaWupzqpSJ9YrVZOnDhx0/M2m81pBQVPduHCBbp06eI09QOArwMIyRmNzWYn+9z1j0RGjjehjYtPPA9vYoIbqrixutDGJk5e0pJfeP1gRo43oY2K1+PJQhqasdlVZJ+7/mBZxilvQkuYqF+Sa5+60CZmMk/ruHFrkszT3oQ2MbmwtaIqlTY+HT9+nNOnTzNixAi6d+/OE088wfnz5+nevTunTp0CiuZojh07loSEBCZOnOjOy3ApV/XJ4cOHCQkJ4e6770atVtOmTRt69erF999/7+Yrqjw3/v8HGDt2LODa36t7772XNWvWsGPHDqZMmcLp06fp2LFj5V+cG4U0KCFm3+RvXDGq6zH7Jqep7O2lPC5Z1ev19O3blwULFmA0GtmzZw+bNm1i0KBBxcpu3LiRvLw8FEXhwIEDLF++nD59+gAQFxeHRqNh2bJlmM1mVqwoGk2Lj4936/W4Qln65PPPP+fixYtA0bzdxYsX07VrV6DoW/qWLVsoLCzEYrGQlpbG7t276dKli1uvp7IcOnQIf3//4skqoNcp9L3zKgvWB2E0q9hzzIdNv/kxKOZKsbJJXfLYeNCf9NPeWGzw7sYgOrc2EuBrp3UDC3c0NZGyIQiTRcWGX/zJPONNv475xerxZHpvhb5RV1mwOgijScWeIz5s2u/HoLuK99fG/X7kGdQoChzI9mH55nr0iSwaHYlrZ0SjhmWb62K2qFjxXV0A4sON7rwcUYlKG5/CwsL47rvvSE1NJTU1lblz5xIUFERqaipNmjQhPz+fRx55hJiYGJ5++ukquhrXcFWftG/fnuPHj/PTTz+hKAonTpzgu+++Izw8vIquzLUsFgsHDhygadOmjmNHjhwBXNeHAAcPHsRms3Hp0iVefPFFEhISaNu2rfsu1A303gp9I6+yYO3/YvZRHzb94segLiXE7F/8yDP+L2Yf92H59/Xo07EoZh8+oyP9lDc2OxhMKl5LbUDDulbalibprQCPnEg3a9Ysnn/+ebp160bdunVJTk4mLCyM3bt3M27cOPbt2wfA119/zcyZMzGbzTRq1Ihx48YxZMgQAHQ6HSkpKbzwwgu88cYbtG3blpSUFI9ctgpK3yd79+7lrbfewmg0Ur9+ffr168eTTz7pqOedd97hySefRKPR0KpVK9566y06dOhQRVflWpmZmbRr1+6m52clneP5zxrTLbktdf1sJCedI6yxmd1HfRn3YTP2vVI0cb9raAFP3XuB8UuaUWhR0TmkgDeGX18x4c0RZ/jbp43p8lJbmtSzsmDkmRq3bBXArIfO8fzyxnSb8b/+eugcYU3N7D7sy7iUZux7u6i/vt4dwMzljTFbVTSqa2XcPZcY0rUoQOq8IOWxHF5Y3og3UoNp29hMymM5smxVDVOa+OTl5UWDBg0cr6lTpw5qtdpxbMOGDfzyyy9kZWWxcuVKR7mvvvrKKZnxFK7ok5YtW/LKK6/wyiuvcPr0aQICAkhMTOSBBx6oqstyqWPHjtG0aVO8vUt+4NIVfQjwyiuvkJGRgVar5d577/XYudC3M+uBczz/38Z0e6EtdfU2kh84R1gTM7uP+DJuUTP2zftfzN4bwMyPb4jZfS4xJK4oZl+4qiH580aczfXCV2cnunUh7407jbYS56sCqBSlsgdvXSM2NrZKdkNx93uWhfRJ2Tz33HMEBAQwc+ZM5xOr3Li7B3j2DlYgO1iJUpH4VDJ3t9ET+uRm1qxZQ0pKCmvXrnUck9+rcpAdrITwHJmZmTXm9pgQQtR0ErPFNR5zs83Hx8ft+/Pebp2/qiZ9UjYlTQNQbAWo3DzSqdgKUGl83fqerqLYClC5cbTTk/uqtpP4VDJ394sn9MnNZGZmEhUV5XRMfq/KRrEVoKqk0c6bvl8lxGyPSVYLCwurZOi/OpM+KT2bzcbRo0eLbcOn0vhiuRTp1rZo6x9w6/u5kkrjy2lm3r6gizTTvOK29xKuJfGpZO7uF0/ok5s5dOgQDz74oNMx+b0qG5XGl9wC981hruv7eaXU65HTAHJzc5k8eTJRUVH07t2b1atXl1hu4cKFdOjQgejoaMfPjcswpaenk5SURKdOnUhKSiI9Pd1dl+Bype2TpUuX0qdPH2JiYujRowevvvoqVqvVcX7kyJHEx8cTExPDwIED2bhxo7suoVJlZ2fTqFEjfH1v/o0vL09hyrMWuvQ20XeIia/WlfxQ1JIVVgaPMBPXx0S/JBNLVlidzp8+ozBmspnYP5lIfNDMTztr5pqhV3LNvDh5G/dFrWRY76/ZtLrkJdAURWHxvAMMvmsVg+9axeJ5B5yWjMtKz2VC0kb6d1rJhKSNZKXnuukKhLuUNj5dYzab6d+/Pz179nQcO3bsGBMnTiQ+Pp64uDgeeeQRjh49WtlNr1SuitvX7Ny5k/DwcN56663Kbrpb3O6h2NL23/bt2xk5ciSdO3cmISGh2Pm3336bxMRE2rdvz8KFC13W/uomL8/KjKey6RX/C4P6p7Pu65K3Kt69K5+Jjx4hocdBBvcvnhflnDYz8dEj9Iz/haGDM9m5/WplN90zk9U5c+ag1WrZunUr8+bNIzk5mcOHD5dYtn///uzbt8/x06JFC6AoGE6aNImBAweya9cuBg8ezKRJkzCbPXM9zNL2SUJCAitXrmTv3r2sWbOGjIwMli9f7jg/c+ZMfvzxR/bu3cvLL7/MM888w7lz59x5KZXi0KFDt537NPcNK1otbPlKx+vJWl6eZyXraMmJ5qsvebFtnY5Fb2n57xc2vt5wPbGd8ZKFO9qp+fEbHVMe0zBtpoVLl2veg0L/nLMPrVbNF1sTmTkvjreT93LscF6xcms+PcaPG3N4P+3PvL+qLz9tPsPqT4qSDIvZzguTtvHngS1J2zWQfoNb8cKkbVjMNTPBr63KErMBPvzwQ+rXr+907OrVqyQkJPDNN9+wdetWOnbsyKRJkyq76ZXKVXEbipZ5euWVV+jUqZO7ml+prly5wtWrV2nWrNlNy5S2//R6PX/5y1+YMWNGifW0atWKp59+ml69erms/dXRvL+fRqtVsfbb9sx+tSWvv3qao1nFd+ry9VGTOLg+TzxZ8k5oL/7tBOERvqz/rgOPPd6Yvz19nMuXin95ciWPS1aNRiPr169n6tSp+Pn5ERsbS0JCAmlpaWWqZ+fOnVitVkaPHo1Op2PUqFEoisL27dsrqeWVpyx90rJlSwIDA4GiES+1Ws3x48cd5yMiIvDyKpodolKpsFqt/P7778Xq8TS3m6hvLFDYsNnOE+M16PUqYjqp+dPdalZ/UzxpGvt/XrQPV+PlpaJ1KzW979bw84Gictkn7PyWqTD5UQ0+Pir69tYQ1lbFhs01K/kqMFr5Yf0pxkztgK+fFx1jg+ma0JQNacVHV9elZjN0bDsaNNbToJEvD4xpx7qVRb9zP+88h81q56+jw9DpNCSNCgNFYd92z/+CJIqUNWafPHmSVatWMX78eKfjkZGRPPDAA9StWxetVsvDDz/MsWPHuHy55NGh6s6VcRvgo48+onv37rRp08Yt7a9shw4dIiws7KbbfZel/yIjIxk8eLBjsOqPhgwZQq9evTxyB8vSKiiws3njFSZMboxeryEq2o+7ewWy9qvin58OHfXcN6AezZoXX8rzxHETmekFjJvYCB8fNQl/rkPbMB82byo+UOFKHpesZmdno9FoaN26teNYREQEWVlZJZbfvHkzcXFx3H///Xz88ceO41lZWYSHhzstEB8eHn7TeqqzsvbJ6tWriYmJIT4+noyMDIYNG+Z0fsKECXTs2JEHHniAuLg47rzzzkptvzvc7nbS8RMKXhoIaXn9IxEeqiLr6K1HRBVFYe/Pdtq2Lnpd1lGF5k1V+Pnd8HsVquLIsZqVrJ7KvopGo6ZF6wDHsbYRdcjOKr7A9PHDV2gbUce53OGictlZV2gbXsfpc9gmvOR6hGcqa3yaO3cu06ZNu+1DLbt376ZBgwbUq1fPpe11F1fG7dOnT/PFF18wefLkSm+3u9wuZpe1/2q7E8dNaLygZavra9aGtfPl6JHiI6u3cvRIIc2a6/Dzu76wannqKSuPecDqGqPRiL+/v9OxgIAAx37BN+rfvz9Dhw4lODiY/fv3M2XKFAIDAxkwYAAGg4GAgACn8v7+/iXWU92VpU8AEhMTSUxMJDs7m9TUVIKCgpzOv/fee1gsFrZt28bRo0dv+s3Wkxw6dMixIURJjAXwxy/VAf4qDMZbJ5kpH9iwKzBkgNpRT4Dz/wr8/VWcO1+zpgEUGK3o/Z3Dh1+AlgKDpcSyfv5a53JGK4qiUGiw4hegdSrv56/FWEI9wjOVJT5t2LABm81G37592bFjx03r/P3335k9e7ZHL97uyrg9d+5cxwhjTXG7qVtl7b/azmi0OyWYAP7+aoyGsg2kGI12/Pz/WI+G8+cqN2Z7XBai1+vJz3feujI/P7/ED2loaCiNGjVCo9EQExPDqFGjWLduHQB+fn7F6jEYDB75YS9Ln9woJCSEsLAwZs+eXeycVqulV69e/Pjjj2zatMml7a0Kt/uWrveFP8a4fIOCn/7miyl//LmN1WttvPuGFp1O5agn/w/1GAwKfvpyN71a8tV7Ycx3nqNkzLfi66ctsazBYHUup/dCpVLh4+eF4Q/1GAxW9CXUIzxTaeOT0Whk3rx5vPDCC7es79KlS4wdO5bhw4czYMAAl7fXXVwVt7/99lsMBgP33XdfpbW1Ktw2Zpez/2orvV6NweD80LDBYEfvV7Y0sOR6bGWup6w8LlkNCQnBZrORnZ3tOJaRkUFoaGipXn/tKeTQ0FAyMzOdnkrOzMwsdT3VSUX6xGq1cuJEyU9xQ9GSTzeuoOCJ8vPzuXTpEi1btrxpmVYtVVhtcPzk9W+ZmYcVQtuUnKx+udrGh8utfLBQR+OG18uEtlFxKkfBYLjh9ypLcUwTqCmahwRgs9k5lX39KdAjGbmEhAYWK9sqLJAjGbnO5cKKyoWEBnI0M8/pc3g0M6/EeoRnKm18On78OKdPn2bEiBF0796dJ554gvPnz9O9e3dOnToFQF5eHmPHjiUhIYGJEye68zJczlVx+6effuLgwYN0796d7t278/XXX7Ns2TKP75/bjaxWNBeobVq28sZmLZoOcM3hQwW0aVu2NWTbtPUh55TZKWE9fKiwzPWUlcf9BdXr9fTt25cFCxZgNBrZs2cPmzZtYtCgQcXKbty4kby8oj+EBw4cYPny5fTp0weAuLg4NBoNy5Ytw2w2s2LFCgDi4+Pdej2uUJY++fzzz7l48SJQNG938eLFdO3aFYAjR46wZcsWCgsLsVgspKWlsXv3brp06eLW63G1w4cPExoaesvpDHpfFX/+k5p33rdhLFDYu9/O5h/sJN5b/DVr1tn45yIrixdoadHMOZkNaakmIkzFux/aMJkUNn5n41CWQt/eHvdRuyVfvRd3923GRwt+o8Bo5eCeC2zblEPfQcW/ENwzqBX/76PDnD9bwIWzBXz20WH6DWkFQFRcQ9QaFV8uy8JstrFyRdF8s+j4hm69HlF5ShufwsLC+O6770hNTSU1NZW5c+cSFBREamoqTZo0IT8/n0ceeYSYmBiefvrpKroa13FV3J46dSrr1q1z9FtCQgIPPPAAf//73916Pa6kKAqHDh267chqafvPbrdjMpmwWCwoioLJZHJa+cdisWAymVAUBavVislkwmYreelCT+Xrq+ZPfQJZ/K/fKSiws3+fge+/u0L/+4vP+bbbFUwmO1arggKYTHYslqKBnJatvAkL9+WD985iMtn57ts8sg4V0LtPnWL1uJJH/gWdNWsWhYWFdOvWjenTp5OcnExYWBi7d+8mOjraUe7rr7/mnnvuISYmhhkzZjBu3DjHvEWdTkdKSgppaWnExsbyxRdfkJKSgk5X/Ok3T1DaPtm7dy+JiYlERUUxfvx4evbsybRp0xzn33nnHbp27UrXrl1ZtmwZb731Fh06dKiKS3KZ291OuubFp70wmaDXfWZmzLLw4jNehLZRs+dnO10Srn8bXfielbw8GDbWQpcEE10STMx+/fp8nXkva/k1w063e8y8/S8bb76ipX49N+7N7CZTZ8VgLrTxl26rmTt9B08mx9A6rA4Hdp/nvuiVjnKJw9rQtXcTHk1czyOJ64nv1ZjEYUVPLGt1al5O6cr6tOMMjE1j7RfZvJzSFa3OI0OTuInSxCcvLy8aNGjg+KlTpw5qtZoGDRqg0WjYsGEDv/zyC19++aXT2tk5OTlVfHXl54q47e/v79RvPj4++Pr6Urdu3Sq6qoo7ffo0/v7+1Klz6wSotP23a9cuIiMjGT9+PDk5OURGRvLII484zr/44otERkayZs0aFi1aRGRkZJlXGPIEM55vhqlQ4d7ev/Li307w7PPNaBPqw769Bv7U9aCj3L49BnredZCnHs/m9zMWet51kCmPHXOcn/taSzJ+LaBvz19J+ecZ/j6/FfXqV+4jUCrlxvtv1VhsbGyV7Frh7vcsC+mT0pkzZw4mk4lXXil5NyTZwaps3LqDFbKDlaeS+FQyd7fRE/rkj7799ltmz57Nli1bip2T36uykx2shPAApR1ZFUIIUfUkZos/kmRV1Hil2b1KCCFE9SAxW/yRx6yz6uPjQ2xsrNvfszqTPrk9RVFu+S1dUQrdflteUQpRqTyrH69RFAvNVO67Na8oFlQqWcbKE0l8Kpm7+8UT+uSPMjMzSUhIKPGc/F6VjaKYKu3W/M3eT6Xyvn3BMvKYZLWwsLBK5qlUZ9Int/f777/j7e1dbJ/xa1QqH5mzWgYqlRa+dd/DYqoEj5hSL0og8alk7u4XT+iTP7rVAIP8XpWNSuUNGe7b5U0VUTnbH3vkNIDc3FwmT55MVFQUvXv3ZvXq1SWWUxSFefPmcdddd3HXXXcxb948p/Uc09PTSUpKolOnTiQlJZGenu6uS3C50vbJ0qVL6dOnDzExMfTo0YNXX30Vq/X6ouwjR44kPj6emJgYBg4cyMaNG911CZWiLLeT8vIUpjxroUtvE32HmPhqXclLlyxZYWXwCDNxfUz0SzKxZIXzovanzyiMmWwm9k8mEh8089POmrXV6jW5BjWTFzUlamoovWe2ZvXOgBLLLVwTRIfJYUQ/Ger4OXn++mhp+klvkl5tSacpoSS92pL0k67/Vi6qVmnj0/bt2xk5ciSdO3cuNrKWk5PjtApAdHQ04eHhLFmyxB2X4HKu6BMo+js2fPhwOnfuTM+ePUlJSansplcqk8nE6dOnadOmzW3LlrYPrzGbzfTv35+ePXs6HX/xxRfp168fERERfPnllxVqf3WWe1XF5FfrEDW0Ab0fDWL1lpJj7dI0X/qMDyJmWDA9Hg7i1Q/8sd7w5zBhXBCRDzQg+sFgoh8MZuysupXedo8ZWb3RnDlz0Gq1bN26lfT0dCZMmEBERARhYWFO5T799FM2btxIWloaKpWKMWPG0Lx5cx566CHMZjOTJk1i9OjRDB8+nE8++YRJkyaxbt06j1y+qrR9kpCQQFJSEoGBgeTm5jJlyhSWL1/OmDFjAJg5cyahoaF4eXmxf/9+Hn74YdatW0fDhp657mVZJurPfcOKVgtbvtKRcVhh0nQL4WEqQtsU/0736ktetGur4uRphfFPWmjcSMV9fYu2oJvxkoVOd6r51xsafvjJzrSZFr76TFfjlq+a80lDtF4KW18/QvopbyakNCOiuYmwpuZiZfvHXmX+mN+LHTdbYdKipoxOuMzwnnl88kMdJi1qyrrZx9B5ZHQSJSltfNLr9fzlL39hwIABvPfee07nmjZtyr59+xz/PnnyJPfccw/33HOPW67B1VzRJwDTp0+nb9++LF++nNOnTzN8+HAiIiIca4p7miNHjtCyZUu02ttP/yltH17z4YcfUr9+/WJbskZERHDfffcxb948l1xDdTXnvYCimP3vC6Qf82LCy3WIaG0lrKXzwExCnJmkPoUE+ivkXlUx5fU6LF/jy5hBBY4yi2bm0i3Kfdtie9zIqtFoZP369Y59kGNjY0lISChxTbTU1FTGjh1L48aNadSoEWPGjGHlyqL1H3fu3InVamX06NHodDpGjRqFoihs377d3ZdUYWXpk5YtWxIYWLQ7kKIoqNVqjh8/7jgfERGBl1dRlqBSqbBarfz+e/Ekw1OUdmTVWKCwYbOdJ8Zr0OtVxHRS86e71az+pvio6Nj/86J9uBovLxWtW6npfbeGnw8Ulcs+Yee3TIXJj2rw8VHRt7eGsLYqNmyuWaOrRpOK9fsCmJp4AT8fhdjQQhIiDaTtKNvOUzsP6bHaVIxOyEWnVRiVkIuiwPbMGrY/bS1WlvgUGRnJ4MGDadGixW3rvbZGdvPmzSuj2ZXKlX1y+vRpEhMT0Wg0tGzZkpiYGLKysir7EipNqWN2GfoQir7crFq1ivHjxxc7N2LECLp27Yq3d829q2MshPU/eTN1hAE/X4XY9hYS4sykbS4+H7dlExuB/kV3oRUF1Co4fkbj7iY78bhkNTs7G41GQ+vWrR3HIiIiSvxwHj58mIiICKdyhw8fBop2AQkPD0eluj7aFR4e7pEf8rL0CcDq1auJiYkhPj6ejIwMhg0b5nR+woQJdOzYkQceeIC4uDjuvPPOSm1/ZcrMzCxV4Dt+QsFLU7QD1TXhoSqyjt56zqSiKOz92e7YTjXrqELzpir8/G74vQpVceRYzUpWs8/p0KgVWje6/s06ormJrDMl35XYfMCPuOltuX9OKz7ecn2h76wzOsKbmbjhY0h4MxNZZ2ruH43apqzxqTQURSE1NdWxyYuncWWfjB49mtTUVCwWC0ePHuXnn3927G7liUobs8vah3PnzmXatGke/bBURWTneKFRQ+tm10dRI0IsZJ0s+RbW6i3exAwLJn5kAzKyvRjWr9Dp/NNv1iF+ZNEUgIxjlX8bzONutBmNRvz9/Z2OBQQEFBvWL6lsQEAARqMRRVEwGAwEBDjPsfP39y+xnuquLH0CkJiYSGJiItnZ2aSmphIUFOR0/r333sNisbBt2zaOHj16y21KqwuloADbsWNYs7OxnziBYjaDRsOfCgqIVKux5+WhvsVuKMYC8PNzPhbgr8JgvHWSmfKBDbsCQwaoHfUEOP+vwN9fxbnz1edBIUVROGrMY//VC+zLO8fvJiMKCnW8vImu05DIgGDa+9fH6xb/342Favx9nfsmwNeOobD4a/p3vsrQHrkEB9rYf8yHKYubEqi3M6DLVQwmNQG+zreg/G9Sj/BMZY1PpbFnzx4uXrxIv379Ktq8KuHKPvnTn/7Es88+y5IlS7DZbEyePJnISPc+NOpKMTExpfqbU5Y+3LBhAzabjb59+7Jjxw6XtdWdzpqM/HzlPD/nnedoQR4Wuw0/jZaOgcF0DAgmMiAYP6+bT50wFqjw1/8hZvspGApKnp6W2MtEYi8T2TkaUjf7EFT3+mvnTbtChzYWFGDZaj2PJNdhbcolx2hsZfC4ZFWv15Ofn+90LD8/H78/Zhr/K3vjL25+fj56vR6VSoWfn1+xegwGQ4n1VHdl6ZMbhYSEEBYWxuzZs3nnnXeczmm1Wnr16sWyZcto2bJltZ3/ZD16FNO6dVj37QMvLzCb4YY9nR9u1gyvtWu5uno1mjZt8O7fH6/ISFR/CIZ6X/hjjMs3KPjpbz7P9OPPbaxea+Pfi3TodCpHPfl/qMdgUPCrBne1861mvjiTxb9OHOCiuRC1Cgw254fDVp09ikalQqNSMarZHYxu0Z7G3iV8tnzs5Bc492F+oRo/n+LJfWiT63NYY9oWMirhMuv2+jOgy1X8vO3kFzrfXjLcpB7hmcobn25l5cqV3HPPPR4Zr8F1fZKbm8ujjz7KSy+9xIABA7hw4QJTpkwhKCiIESNGuLLJbtOnTx/s9tt//kvbh0ajkXnz5rF48WKXttMdbIqdDRdO8G72AX7Nv4hOpcZgs3Bj73x17hjeag0Wxc79DVszoWVH2gcEFatL76uQb/xDzDaq8PO9dYIZ0tRGWAsrsxf5887frgDQ+Y7rd9Qm/NXIym992P2bloS44s8ruIrHDV+EhIRgs9nIzs52HMvIyCA0NLRY2bCwMDIyMpzKXZt4HRoaSmZmptPqAJmZmSXWU92VpU/+yGq1cuLEiZuet9lsnDx50hXNdCl7fj6Gd97B8PrrWHftAosFCgqcElWgaHSwsBCsVmyHDmFctIj85GRsf5iH26qlCqsNjp+8HgYyDyuEtik5Wf1ytY0Pl1v5YKGOxg2vlwlto+JUjoLBcMPvVZbimCZQVTZeOMFdWz/hlaydnCrMp8BuLZaoAhTYreTbLORZzSw++Qt3b/uMRccPYFOc/3iENDRjs6vIPnf9m3zGKW+nxPRWrvVOaBMzmad13Ljpc+Zpb0KbmMp8jaJ6qkh8KklhYSHffPMNgwcPdk0Dq4Cr+uTkyZNoNBoGDx6Ml5cXjRs35r777uP77793cYvdR61WO56buJXS9uHx48c5ffo0I0aMoHv37jzxxBOcP3+e7t27c+rUKVc332Uy8y/TZ/uXTP31O/ZeOYfJbuPqHxJVALNi56rNQqHdRurvRxi0exVP/vodV63OsTikqRWbHbJzrg8OZBzzIrRF8b8Df2S1w4nfbz5nVaXCKYZXBo9LVvV6PX379mXBggUYjUb27NnDpk2bGDRoULGygwYN4qOPPuLs2bOcPXuWjz76yDHHKS4uDo1Gw7JlyzCbzaxYsQKA+Ph4t15PRVybDF6WPvn888+5ePEiUDRvd/HixY75TUeOHGHLli0UFhZisVhIS0tj9+7ddOnSxX0XVQrWw4e5+txzWPfvLxpJLcunxGTCfuoU+S+9hOm77xyH9b4q/vwnNe+8b8NYoLB3v53NP9hJvLf4R2TNOhv/XGRl8QItLZo5J7MhLdVEhKl490MbJpPCxu9sHMpS6Nu7aj5qFrudJw5uZuIv35JnNWO03z4wXWOy2ymw23jz6F7u35XGRfP1J0H13gp9o66yYHUQRpOKPUd82LTfj0F3XSlWz8b9fuQZ1CgKHMj2YfnmevSJLBp+jmtnRKOGZZvrYraoWPFdXQDiw40Vu3BR5a49D1CW+GS32zGZTFgsFhRFwWQyYTY7/9HdsGEDderU8ahY/Ueu6pPWrVujKAqrV6/Gbrdz/vx51q5dW6N3f7o2RaC0fRgWFsZ3331HamoqqampzJ07l6CgIFJTU2nSpAlQtKSVyWRCURSsVismk6lUo7uV5cOTB7l/VypHjLklDircjA2FAruN1WeP0WPbZxy4csFxTu8DfeNNLPjYD2Mh7EnXsmmnN4N6Fxar5/P1PlzMLfr8Zp3QsPj/+dE1smg0Nee8mj3pWswWMJnhgy/1XL6iJuaOyl0ZwGOS1Rsn0s+aNYvCwkK6devG9OnTSU5OJiwsjN27dxMdHe0oN2zYMHr37u2Yo9mrVy/Hw0Q6nY6UlBTHE6VffPEFKSkpHrVs1aJFixz/Xdo+2bt3L4mJiURFRTF+/Hh69uzJtGnTHOffeecdunbtSteuXVm2bBlvvfUWHTp0cOt13Yo1PR3D/PlF9+ytpf8QO1EUsFgo/O9/KfzmG8fhF5/2wmSCXveZmTHLwovPeBHaRs2en+10Sbg+0rfwPSt5eTBsrIUuCSa6JJiY/fr1D+q8l7X8mmGn2z1m3v6XjTdf0VbJslUWu52RP3/DN+ezKShDkvpHRruVzPzL9N+ZynnT9SRy1kPnKLSo6TajLdM/bELyQ+cIa2pm92Ffop+8Prrx9e4A7pnVmpinQpmxtDHj7rnEkK5FSa3OC1IeyyFteyCx09vyxbZAUh7LkWWraoA5c+Y4/ru08WnXrl1ERkYyfvx4cnJyiIyM5JFHHnGqNzU1lYEDBzo9HOtJriVIrugTf39/Fi5cyNKlS+nSpQuDBw8mLCyMiRMnVsm1ucONy0uVpg+9vLxo0KCB46dOnTqo1WoaNGiARlM0WvjII48QGRnJvn37ePHFF4mMjGTXrl1Vcn1vHd3La1m7KbTbKO9gpUmxcdFSyF/3rmFP3jnH8VmPXaXQrKLbqAZMnx9I8mNXCWtpY/evWqIfDHaU25uuJXFKEFFDGzD+5br07Gxi2siiKReGAhXJ/wogbkQDeo4N5od9Ot5/KZd6gZU7tKpSlMoevHWNgoICfH193fqesbGxbt8poywKCwvd/mRjVfaJ7cwZ8mfPBpMLbxHrdOiGpePVKdd1dd6Gu3awmvLrd6w9d4wCe8mbG5SVFypC9IGsNz6ETuWaOm9LdrDyWBKzS3b27FkaNWrktvfzhD4pi5r8e/VZziFmZm6r0ODCH/lrtGwInk0Lr8rZWaqY2r6Dlbt/OT1BbVqCQ7HbMb77btFtf1cymzF/GoJytWYN5W26cMKliSqAFYXThQbeZKDL6hQ1l8TsktW5xaok4vZq6u9VTmE+L7g4UYWi5xAmXxpR6XNKK5tH/YU2mUxs2rSJl156yS3vd+12TXVnMBhITEzkypXi8wVdrar6xLR+PfZz5ypnFrdVhyV1NH5Tp7q+7hIoiqlov+ZKYrRZmPrrFpcmqtcU2K18oE5kcJcPiPCv7/L6i7EVgqb2fCmraex2O7///juDBw92yxxAT4jZPj4+FBQU8MQTT/Dzzz9X+vt5Qp+Ux8WLFxkwYAAWS+XvouSOPnzyty2YFdfHbJuikGFry38Df2J4s4jbv6Ci7IWgdn3M9phpAKL2Umw2rk6dilKZa+BqtQS8+irq4ODbl63m/nMqndlZOzCWYWJ+WWhQkdioDe/c2btS6hdCiNrksKHomYDCShhguKaxt55d3R/y2LneVTINICQkhI0bNzodW7p0KT169KiK5lQrS5cupWPHjuj1eho3bszEiRPJzc2t6mZVKev+/Si20n+IPz96lD+tXk2zFSsI//RT/rphAz+dPXvrF9ntmP7wO+mJFEXh3RMHSpWo/j7iOQr3/OZ0zLBuK+envn7L19lQWHs+m1yLLC9VW0jMvjmJ2RX38ccfExsbi7+/P02aNKF///78+OOPVd0st3n/xEEspbjzUN6YDXDVambb5TPlbmNV85g5q7XBG2+8wbPPPsu8efPIy8tj+/btHD9+nL59+xZbvqU2MW/bVrRWaim88+uv/G3nTqZFRnLowQc5+MADPBIRwde3WEsWAJsNy/btLmht1TpReJWzpspf9slLpWLzxeq3/q4Q7iQxu+LefPNNnnzySZ5//nnOnj3LiRMnmDRpEmlpaVXdNLdZc+4Y5X/2v3QMNitf/u5528lfI8lqNXHlyhVmzZrFwoULuffee9FqtYSEhPDZZ5+RnZ3tWAe2NrIdPVqqcnlmM3/ft4/58fEMbNUKP60WrVpN/xYteLkUa8Uq+fkoRs9e3/PAlQt4qSr/Y220Wdl7w5IoQtQ2ErMrLi8vj5deeomUlBSSkpLw8/NDq9WSmJjotERVTXbWZKSwDHcOK2J33m3uMFZjkqxWE9u2baOwsJCkpCSn4/7+/tx3331s2LChilpWtRSTCSUvr1Rld507R6HNxoCWLcv3ZjodttuNwFZzP185j8FW+Q8cKMAuDw58QlSUxOyK++mnnygsLHRaR722OXj1It5q96RixwuuFtuN0FNU2WoA17aHu8ZsNhMTE1Oq15pMJtatW1cttwH9owcffJDgUjy0c+HCBYKDg0vcZq5Jkybs2bPntnVkZ2ezZs2acrWzuvI1mxlM6X5RL5lMBHl7F22xWg5mk4nvPv+ck/Xqlev11cEPTb1Q6pT+Y31p1ruguaG/rDa0oaVL9i/LnNVapSIxG4oWt9+5c2dlNM2lunbtWqrrckXMNplMvP/+++VqZ3U2duxY9Hr9bctdvHjxpn1YWgcOHPDo7WV/qaOmsJEWNKV78KkiMVtF0V2xAC/P2fzomipLVlNTU/nzn//s+PfSpUv54IMPSvXaa/vV//bbb7cvXMVKO28pODiYCxcuYLVai31wz5w5U6qE12g0kpGRUa52VleBdnupl7yp7+3NRZMJq91eroTVbreTc/o0Gbd7GKsay/NvBXVKn2zXnz0Jn87tHf82rNuK8evSPdggC4nULhWJ2VCUmHhCzC7tVqWuiNl2u73GxWwo+htdGkFBQTftw9LKzc316D4807Ie9gbN4X+7ad1ORWI2gN1D47ZHrbN6jV6vZ/LkyVXdDJfq2rUr3t7efPnllwwdOtRxPD8/n7Vr1/Lqq6/eto727dvzzjvvVGYz3U4pKODKE09AKYJfl4YN8dZo+OrECQaFhJT5vXx8fXl08mS01Wh72bJ6MXMbH51yT0Lgid/ORdW59957uffee6u6GS7jipjt6+tb42J2WVzrw9TUVP7617+Wq46ePXvSs2dPF7fMfb45n81Tv27hqhumb9kVBb1GW+nvUxlkzmo1UadOHWbNmsUTTzzBN998g8ViITs7m6FDh9K8eXNGjhxZ1U2sEipfX1SluJ0EUEen429RUTy9fTtrjh/HaLVisdvZcOoUL5VmqzyLBU2LFhVscdXqFNgAvcY930GjAhu45X2EqI4kZldcnTp1mDNnDpMnTyY1NRWj0YjFYmHt2rXMmDGjqpvnFu39g7C6abSziY8fWjfNj3U1jxxZralmzJhBUFAQTz/9NEeOHCEwMJDBgwfzn//8B2/vytvxqLrTtGqF9eDBUpV94s47aeTry/wDBxj/ww/4e3kRFRzM9MjI275W5e2NOjCwos2tUh0DgnHHks96tRdd6rpvf3MhqiOJ2RU3ffp0GjduzNy5cxkxYgQBAQF07tyZmTNnVnXT3KKFj7/b3ivagwcYZAcrUe2Zf/yRghUrwFSJD/So1Wi7dkX/6KOV9x5uYFcUon/4DxctpVuXtry81Rp+6DqUpj5+lfo+QghR0z16YAPrzh+v1JVW/TRa3rjjbgY0alOJ71J5ZGRVVHvauLiiZLUyeXnh3a9fscOJiYmcOePeXT+aNGnC6tWry/VatUrFoy3u5J/Z+yp167676jaWRFUIUe14WswGeKxlJN9fPI3RXjlbZANoVCr6NQiptPormySrotpT6XToevfG/O23UBm7wqhUaJo1K3G+6pkzZ9hdmvmuLhQbG1uh1w9vFs6C7J9d05gS+Kq9eCIkqtLqF0KI8vLEmN25TkOa+fqTZcitlNFVX7UXj7a402Pnq4I8YCU8hM/gwaV+0KrMtFp8x4+/bbHc3FwmT55MVFQUvXv3vuk36YULF9KhQweio6MdPzeuCZyenk5SUhKdOnUiKSmJ9PR0l10KQJDOl5fC4tCrXf9dVKdWc0+DlnSt18TldQshhCu5KmaHh4cTFRXlOOfq+bQqlYqUDr3xVpdu+aqyCtb5MDmkU6XU7S4ysio8gsrbG/3EiRjeeMO1o6s6Hd6DB6Np3Pi2RefMmYNWq2Xr1q2kp6czYcIEIiIiCAsLK1a2f//+zJ8/v9hxs9nMpEmTGD16NMOHD+eTTz5h0qRJrFu3Dp3OdUtB/V+zO/jy9yPsv3Ies4t2LFEB/hodr4Z3d0l9QghRmVwRs69JS0ujVatWldbW9gFBTGgZyeITv1DgwukAPmoNizr2qbRE2F1kZFV4DK927fAZMQJcldTpdGjj4vAuxdqPRqOR9evXM3XqVPz8/IiNjSUhIYG0tLQyveXOnTuxWq2MHj0anU7HqFGjUBSF7du3l/cqSqRSqVja6R6a+fijU1X8Y66iaIL+5zH3U0crTzkLIao3V8Vsd5reJoY+wS3wddFdMR+1hjfu6EknD14F4BpJVoVH8e7ZE9/Ro0GrBVUFFmnS6dDdfTe+Y8agKkU92dnZaDQaWrdu7TgWERFBVlZWieU3b95MXFwc999/Px9//LHjeFZWFuHh4U7vGR4eftN6KqKO1ptVXQbSzq9ehaYEeKs11Nf6kBY7kHB/z92KVghRe7gqZl8zYsQIunfvzuOPP86pU6cqpc1qlYqUO3szsFGbCiWsalT4qjX8s30vBjVu68IWVh2ZBiA8jq5bNzStW2N8913s58+XbUkrrRaVtze+EyagvfPOUr/MaDTi7++8Hl5AQAAGg6FY2f79+zN06FCCg4PZv38/U6ZMITAwkAEDBmAwGAgICHAq7+/vX2I9rlBP68NXcYN49/gB/nlsHxa7HVspp/CrAB+1F/0btmJueHcCZccqIYSHcFXMBlixYgWdOnWisLCQt99+m8cee4zU1NRybxF7KxqVmjfa96Rfg1Y89dsWCm1WTGWYyqXXeNFGX4d/3ZlAa30dl7evqsjIqvBImiZN8J89G99Ro1A3bVo0NUB7k23kNBrw8UEVGIj34MEE/OMfZUpUoWiL3/z8fKdj+fn5+PkVX74pNDSURo0aodFoiImJYdSoUaxbtw4APz+/YvUYDIYS63EVjUrNEyFRrL8riaFN2+Gj1uCv0d70w++n8UKnUtM7qDn/ib6XBR16S6IqhHAbVyz/7qqYDdClSxd0Oh2BgYHMnDmTU6dOceTIkQq38VbuadCKrd0e5PGQKOppvfHXaPG6yV1AH7UGX7UX7f3r84+Iu/m6y+AalaiCjKwKD6ZSq9F164auWzdsJ09iPXwYW2YmttOnUSwWVGo13x08SMzQoTSKi0MTFoaqnEt3hISEYLPZyM7OJiQkBICMjAxCQ0NL9fprwTc0NJQlS5agKIpjKkBmZibDhw8vV7vKoo2+DvPuuJvksHi2XDrFvivn2Zn7O5fMhdhRCNDoiK7TkM51GtKjflMae8s6qkII98rIyKCwsJCoqKgK1eOqmF0SlUrlkoT6dupqvXmqTQxTWkex/fLv7M07x0+5ZzhVmI/VbsdHo+EO/yDuqtuIu+o2qdHTtCRZFTWCpkWLonVSExKcjr/Trx9Tg4K4Lzy8QvXr9Xr69u3LggULmDt3Lunp6WzatIlPPvmkWNmNGzfSpUsXAgMD+eWXX1i+fDnTpk0DIC4uDo1Gw7Jly3jooYf47LPPAIiPj69Q+8rCz0vLfQ1bc1/D1rcvLIQQbvT5559TUFBQ4WTVVTH78OHDWK1W2rVr55gG0LBhQ9q2dd9cUI1KTff6TelevylPEOW2961OZBqAqNHCw8PJzMx0SV2zZs2isLCQbt26MX36dJKTkwkLC2P37t1ER0c7yn399dfcc889xMTEMGPGDMaNG8eQIUMA0Ol0pKSkkJaWRmxsLF988QUpKSkuXbZKCCE8VXWL2RcuXODJJ5+kc+fO/PnPf+b06dO89957aG827UxUCpXijrFsIarIO++8w8GDB1m0aFG5Xh8bG1slu6G4+z2FEKI6+Pnnn/m///s/Dh48WK7XS8yumWRkVdRo4eHhHDp0qKqbIYQQohTCwsI4cuQINputqpsiqhFJVkWN1q5dO5fdUhJCCFG5/Pz8CA4O5sSJE1XdFFGNyANWokZr0aIFly9fJj8/v9iae6Xh4+NDbGxsJbTs1u8phBC11bU7Yjcu6F9aErNrJklWRY2mVqsJCwvj0KFDxMTElPn1hYWFVTL/SQghaqtrD1n169evzK+VmF0zyTQAUeO5aipAbm4ukydPJioqit69e7N69eoSyymKwrx587jrrru46667mDdvntOafOnp6SQlJdGpUyeSkpJIT0+vcNuEEKKmcHfM3r59OyNHjqRz584k/GH5Q4BTp04xcuRIOnXqxL333su2bdsq3DZRNpKsihrPVQ9ZzZkzB61Wy9atW5k3bx7JyckcPny4WLlPP/2UjRs3kpaWxqpVq9i8ebNjbT+z2cykSZMYOHAgu3btYvDgwUyaNAmz2Vzh9gkhRE3g7pit1+v5y1/+wowZM0qsZ/r06bRv354dO3bw1FNPMWXKFC5dulTh9onSk2RV1Hiu+JZuNBpZv349U6dOxc/Pj9jYWBISEkhLSytWNjU1lbFjx9K4cWMaNWrEmDFjWLlyJQA7d+7EarUyevRodDodo0aNQlEUtm/fXqH2CSFETeHumB0ZGcngwYNp0aJFsXPHjh3j119/5YknnsDHx4d+/frRrl07p+1YReWTZFXUeK74lp6dnY1Go3Ga8B8REUFWVlaxsocPHyYiIsKp3LVv81lZWYSHhzu2Wr3WvpLqEUKI2qhVq1acP38eo9FY7jrKErNvJSsrixYtWjg9oFueekTFSLIqarx27dpx6NChCu3lbDQai60mEBAQgMFguG3ZgIAAjEYjiqJgMBgICAhwKu/v719iPUIIURtpNBratm1b4i370ipLzL6VkmJ2eeoRFSPJqqjx6tWrh4+PD2fOnCl3HXq9nvz8fKdj+fn5+Pn5lVj2xkCWn5+PXq9HpVLh5+dXrB6DwVBiPUIIUVtVdCpAWWL2rZQUs8tTj6gYSVZFrVDRqQAhISHYbDays7MdxzIyMggNDS1WNiwsjIyMDKdyYWFhAISGhpKZmek0ypuZmVliPUIIUVu5M2bfSmhoKCdPnnRKWMtTj6gYSVZFreCKb+l9+/ZlwYIFGI1G9uzZw6ZNmxg0aFCxsoMGDeKjjz7i7NmznD17lo8++oghQ4YAEBcXh0ajYdmyZZjNZlasWAFAfHx8udsmhBA1jTtjtt1ux2QyYbFYUBQFk8nkWKGldevW3HHHHaSkpGAymdiwYUO514AV5SfJqqgVXPGQ1axZsygsLKRbt25Mnz6d5ORkwsLC2L17N9HR0Y5yw4YNo3fv3iQmJpKYmEivXr0YNmwYADqdjpSUFNLS0oiNjeWLL74gJSUFnU5XobYJIURN4s6YvWvXLiIjIxk/fjw5OTlERkbyyCOPOM6/+eabHDx4kC5dujB//nwWLFhA/fr1K9Q2UTYqpSJPnQjhIdLS0nj//fdZs2ZNmV4XGxtbJbuhuPs9hRCiOrl48SJt27bl8uXLTqun3I7E7JpJRlZFreCqHVGEEEJUvqCgIDQaDefOnavqpohqwKuqGyCEO7Rt25aTJ09iNpvLdMvdx8fH7fs++/j4uPX9hKgsCQkJzJ07l27dugHw1VdfkZycTEpKCnFxcVXcOlHdXZsK0KhRo1K/RmJ2zSTJqqgVdDodzZs35+jRo04L9t9OYWFhldxSEqKmWblyJa+99hrvvfceMTExVd0c4QGu3RG7++67S/0aidk1k0wDELVGRSfs5+bmMnnyZKKioujduzerV68usdzChQvp0KED0dHRjp+TJ086zqenp5OUlESnTp1ISkoiPT293G0SwhN88sknvPbaa3zwwQfExMRw9epVnn/+eXr06MHdd9/NW2+9hc1mw2w2ExcX5zRl5+LFi3Tq1En2Yq+F3BWzly5dSp8+fYiJiaFHjx68+uqrWK1Wx/mEhAQiIyMd8Xzs2LHlbpMoH0lWRa0RHh5eoXmrc+bMQavVsnXrVubNm0dycvJNd1jp378/+/btc/xc23PabDYzadIkBg4cyK5duxg8eDCTJk1yLJMiRE3z3//+lwULFvDvf/+bjh07AvDcc8/h5eXF+vXrSU1NZevWrXz++efodDruu+8+Vq1a5Xj9mjVr6Nq1K/Xr1ycxMfGmCYeoedwVsxMSEli5ciV79+5lzZo1ZGRksHz5cqcyixYtcsTzJUuWlLtNonwkWRW1RkUesjIajaxfv56pU6fi5+dHbGwsCQkJpKWllamenTt3YrVaGT16NDqdjlGjRqEoCtu3by9Xu4So7rZu3UqnTp1o164dABcuXGDLli08//zz6PV6goKCePjhh/nqq68AGDJkCF999ZVj44y0tDQGDhwIwOrVq0lMTKyaCxFu566Y3bJlSwIDAwFQFAW1Ws3x48cr1HbhWpKsilqjIreUsrOz0Wg0tG7d2nEsIiKCrKysEstv3ryZuLg47r//fj7++GPH8aysLMLDw52WYgkPD79pPUJ4uuTkZLKzs5k5cyaKopCTk4PVaqVHjx7ExsYSGxvLSy+95LjN36lTJ3x8fNixYwdHjhzhxIkT9OnTp4qvQlSF0NBQsrOznW7Jl1ZZY/bq1auJiYkhPj6ejIwMx9rY1zz99NPEx8czduxYpx0KhXvIA1ai1mjXrt1Nb9vfjtFoxN/f3+lYQEAABoOhWNn+/fszdOhQgoOD2b9/P1OmTCEwMJABAwZgMBgICAhwKu/v719iPULUBMHBwSxdupSRI0eSnJzM5MmT0el0bN++HS+vkv8EDRkyhFWrVtGgQQP69euHt7e3m1stqgMfHx+aNGnCiRMnaNOmTZleW5aYDTg2ccnOziY1NZWgoCDHuXnz5tGhQwcURWHZsmU88sgjrF271jEaKyqfjKyKWqNp06ZkZ2dTnn0w9Hq9097QAPn5+fj5+RUrGxoaSqNGjdBoNMTExDBq1CjWrVsHgJ+fX7F6DAZDifUIUVM0atSIpUuX8sMPP/DBBx/QvXt3XnvtNfLz87Hb7Zw4cYKdO3c6yg8cOJCNGzeyatUqBg8eXHUNF1Xu119/pVWrVmV+XVli9o1CQkIICwtj9uzZjmOdO3fGx8cHX19fJkyYQEBAgGwC4GaSrIpaQ6VS4e3tXabdUK4JCQnBZrORnZ3tOJaRkUFoaGipXn8tQQ4NDSUzM9MpYc7MzCx1PUJ4qqZNm/Lvf/+bdevW0aJFCywWC/fddx9dunRhypQpnD9/3lG2SZMmtG/fHpVK5bQs0P333+/08JWo+fR6PRqNpsyvq0jMtlqtnDhx4qbnVSpVuQY9RPnJNAAhSkGv19O3b18WLFjA3LlzSU9PZ9OmTXzyySfFym7cuJEuXboQGBjIL7/8wvLly5k2bRoAcXFxaDQali1bxkMPPcRnn30GQHx8vFuvRwh3+Pbbb53+3aJFC7Zs2eL4942jV3/UpEkTOnXq5PTl8tpDWELcTlli9ueff05CQgJBQUFkZWWxePFievToAUBOTg5nzpyhY8eOKIrC8uXLuXz5sqwV7GYysirELdw452nWrFkUFhbSrVs3pk+fTnJyMmFhYezevZvo6GhHua+//pp77rmHmJgYZsyYwbhx4xgyZAhQtDlBSkoKaWlpxMbG8sUXX5CSklKmXbWEqOlOnTrFhg0b+Otf/1rVTREe5sa5raWN2Xv37iUxMZGoqCjGjx9Pz549HQMMBoOB5ORk4uLi6NmzJz/88APvv/8+9erVc/u11WYqRcayhbipQ4cOOZbccZfY2FiZDyU8nmIpRKUt+zaUb7/9Nv/+978ZP348EydOdMt7iprjypUrbn/wSWJ25ZNkVYhbKCwsdPu+zxL4RE1hSy77/PCK0CTLn7PazmazlWuOa0VIzK58Mg1AiFtwd6IqRG3051Wt2fa73unYyqOB/N+GFlXUIuGp3J2oCveQB6yEKIVTp065bQmdJk2auOV9hBCipjKZTAwYMIDLly9X+ntJzK58kqwKUQrNmzeX2zxCCOEhvL292bBhQ1U3Q7iITAMQtUZISAgbN250OrZ06VLHEiVCCCGqD4nZ4hoZWRVCCFHlnvihKV6q6w9IWewq2tczVWGLhBDVhSSrQgghqtzCu3Po1tjo+PfKo4F8caROFbZICFFdyDQAIYQQQghRbcnIqqhVBg8ejJfX9V97s9ks2+YJIUQ1JTFbgIysilomNTWV3Nxcx8+7775b1U0SQghxExKzBcjIqhBCiCq2ceCxYseGtLnCkDZXqqA1QojqRkZWhRBCCCFEtSXJqhBCCCGEqLZUiqIoty8mhBBClJ5iKUCl9a3x7ymEqHySrAohhKgUtmSVW99Pkyx/zoSoieQBKyGEENXCyqOBLM2ox8l8LX5aO39uns9TnS4QqLNXddOEEFVI5qwKIYSoch+l1+PNn4N5Ouo8O/6axSd9T5Bj0PLo5uaYbVXdOiFEVZJkVQghRJXKt6h552AQz8ee4+6mRrRqaOZv5c3uOZw2eLE6O7CqmyiEqEKSrAohhKhS+877YLap6Ns83+m4n1ahZxMDP/2ur6KWCSGqA0lWhRBCVKnLJg11vW14lfAXqYGvjcsmjfsbJYSoNiRZFUIIUaXqedvINWmwlvAc1fkCDfW8ZdKqELWZJKtCCCGqVFRwITq1woZT/k7HDRYVP5zxI76xsYpaJoSoDmTpKiGEEFUqQGdn0p0XeXV3Q/y97MQ3NnLO6MWc3Y1opLcyMORqVTdRCFGFJFkVQghR5R5pf5m63jbm/dyAk/la/L3sJDTP5x/dzqDTyGL/QtRmkqwKIYSoFv7S9gp/aXulqpshhKhmZM6qEEIIIYSotlSKosj9FSGEEC6lWApQaX1r/HsKISqfJKtCCCEqhS1Z5db30yTLnzMhaiKZsyqEEKJaWJMdwL8z63H0ig4/LzsR9UxM6HCRzg0Kq7ppQogqJMmqEEKIKrc0oy4f/FafWV3O0b2JAa1a4cczfnx7yl+SVSFqOUlWhRBCVKmrZjULfwnmlbt+p2+LfMfx3s0M9G5mqMKWCSGqA1kNQAghRJX6+YIPZpuKPzfPv31hIUStI8mqEEKIKpVr1lDX24aX/EUSQpRAQoMQQogqVVdnI9ekwWqv6pYIIaojSVaFEEJUqajgQnRqhU2n/Ku6KUKIakiSVSGEEFUqQGfn8Y4Xmbu7IRtP+VFgVWGxw/c5eubvC67q5gkhqpisBiCEEKLKjbnjMsG+Vt77NYhntzVBr7XToX4hE9pfquqmCSGqmCSrQgghqoXEkKskhlyt6mYIIaoZmQYghBBCCCGqLUlWhRBCCCFEtaVSFEWp6kYIIYSoWRRLISqtT41/TyFE5ZNkVQghhBBCVFsyDUAIIYQQQlRbkqwKIYQQQohqS5JVIYQQQghRbUmyKoQQQgghqi1JVoUQQgghRLUlyaoQQgghhKi2JFkVQgghhBDVliSrQgghhBCi2pJkVQghhBBCVFuSrAohhBBCiGpLklUhhBBCCFFtSbIqhBBCCCGqLUlWhRBCCCFEtSXJqhBCCCGEqLYkWRVCCCGEENWWJKtCCCGEEKLakmRVCCGEEEJUW5KsCiGEEEKIakuSVSGEEEIIUW1JsiqEEEIIIaotSVaFEEIIIUS1JcmqEEIIIYSotiRZFUIIIYQQ1ZYkq0IIIYQQotqSZFUIIYQQQlRbkqwKIYQQQohqS5JVIYQQQghRbUmyKoQQQgghqi1JVoUQQgghRLUlyaoQQgghhKi2/j+Hqx7mIFU/cgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1,2, squeeze=True, figsize=(12, 4))\n",
    "order = [5, 1, 0, 2, 3, 4]\n",
    "time_per_node = 2\n",
    "last_layer = [0]\n",
    "layers = 2\n",
    "input_nodes = np.copy(nodes)\n",
    "def make_frame(t):\n",
    "    axs[0].clear()\n",
    "    axs[1].clear()    \n",
    "    \n",
    "    layer_i = int(t / (time_per_node * len(order)))\n",
    "    axs[0].set_title(f'Layer {layer_i + 1} Input')\n",
    "    axs[1].set_title(f'Layer {layer_i + 1} Output')    \n",
    "\n",
    "    flat_adj = np.sum(adj, axis=-1)\n",
    "    out_nodes = np.einsum('i,ij,jk->ik', 1/(np.sum(flat_adj, axis=1) + 1), flat_adj + np.eye(*flat_adj.shape), nodes)\n",
    "    \n",
    "        \n",
    "    if last_layer[0] != layer_i:\n",
    "        print('recomputing')\n",
    "        nodes[:] = out_nodes\n",
    "        last_layer[0] = layer_i\n",
    "        \n",
    "        \n",
    "    t -= layer_i * time_per_node * len(order)\n",
    "    i = order[int(t / time_per_node)]    \n",
    "    print(last_layer, layer_i, i, t)\n",
    "    mask = [False] * nodes.shape[0]\n",
    "    for j in order[:int(t / time_per_node) + 1]:\n",
    "        mask[j] = True    \n",
    "    print(mask, i)\n",
    "    neighs = list(np.where(adj[i])[0])\n",
    "    if (t - int(t / time_per_node) * time_per_node) >= time_per_node / 4:\n",
    "        draw(nodes, adj, axs[0], highlight=[[i], neighs], labels=['center', 'neighbors'], draw_nodes=input_nodes)    \n",
    "    else:\n",
    "        draw(nodes, adj, axs[0], highlight=[[i]], labels=['center', 'neighbors'], draw_nodes=input_nodes)    \n",
    "    if (t - int(t / time_per_node) * time_per_node) < time_per_node / 2:\n",
    "        mask[j] = False            \n",
    "    draw(out_nodes, adj, axs[1], highlight=[[i]], key=True, mask=mask, draw_nodes=input_nodes)\n",
    "    return mplfig_to_npimage(fig)\n",
    "\n",
    "animation = VideoClip(make_frame, duration=time_per_node * nodes.shape[0] * layers)\n",
    "\n",
    "animation.write_gif('gcn.gif', fps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{glue:figure} dframe\n",
    "----\n",
    "name: dframe\n",
    "----\n",
    "Intermediate step of the graph convolution layer. The center node is being updated by averaging its neighbors features.\n",
    "```\n",
    "\n",
    "\n",
    "```{figure} ./gcn.gif\n",
    "----\n",
    "name: gcnanim\n",
    "----\n",
    "Animation of the graph convolution layer. The left is input, right is output node features. Note that two layers are shown (see title change).\n",
    "```\n",
    "\n",
    "To help understand the GCN layer, look at {numref}`dframe`. It shows an intermediate step of the GCN layer. Each node feature is represented here as a one-hot encoded vector at input. The animation in {numref}`gcnanim` shows the averaging process over neighbor features.  To make this animation easy to follow, the trainable weights and activation functions are not considered. Note that the animation repeats for a second layer. Watch how the \"information\" about there being an oxygen atom in the molecule is propogated only after two layers to each atom. All GNNs operate with similair approaches, so try to understand how this animation works. \n",
    "\n",
    "\n",
    "### GCN Implementation\n",
    "\n",
    "Let's now create a tensor implementation of the GCN. We'll skip the activation and trainable weights for now.\n",
    "We must first compute our rank 2 adjacency matrix. The `smiles2graph` code above computes an adjacency tensor with feature vectors. We can fix that with a simple reduction and add the identity at the same time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 0.],\n",
       "       [1., 1., 0., 0., 0., 1.],\n",
       "       [1., 0., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes, adj = smiles2graph('CO')\n",
    "adj_mat = np.sum(adj, axis=-1) + np.eye(adj.shape[0])\n",
    "adj_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute degree of each node, we can do another reduction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 3., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degree = np.sum(adj_mat, axis=-1)\n",
    "degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put all these pieces together into the Einstein equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0.]\n",
      "[0.2 0.2 0.6]\n"
     ]
    }
   ],
   "source": [
    "print(nodes[0])\n",
    "# note to divide by degree, make the input 1 / degree\n",
    "new_nodes = np.einsum('i,ij,jk->ik', 1 / degree, adj_mat, nodes)\n",
    "print(new_nodes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To now implement this as a layer in Keras, we must put this code above into a new Layer subclass. The code is relatively straightforward, but you can read-up on the function names and Layer class in [this tutorial](https://keras.io/guides/making_new_layers_and_models_via_subclassing/). The three main changes are that we create trainable parameters `self.w` and use them in the `einsum`, we use an activation `self.activation`, and we output both our new node features and the adjacency matrix. The reason to output the adjacency matrix is so that we can stack multiple GCN layers without having to pass the adjacency matrix each time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(tf.keras.layers.Layer):\n",
    "    '''Implementation of GCN as layer'''\n",
    "    def __init__(self, activation=None,**kwargs):\n",
    "        # constructor, which just calls super constructor\n",
    "        # and turns requested activation into a callable function\n",
    "        super(GCNLayer, self).__init__(**kwargs)\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # create trainable weights\n",
    "        node_shape, adj_shape = input_shape\n",
    "        print(node_shape)\n",
    "        self.w = self.add_weight(shape=(node_shape[1], node_shape[1]),\n",
    "                                name='w')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # split input into nodes, adj\n",
    "        nodes, adj = inputs \n",
    "        print(inputs)\n",
    "        tf.print(nodes.shape)\n",
    "        # compute degree\n",
    "        degree = tf.reduce_sum(adj, axis=-1)\n",
    "        # GCN equation\n",
    "        new_nodes = tf.einsum('i,ij,jk,kl->il', 1 / degree, adj, nodes, self.w)\n",
    "        out = self.activation(new_nodes)\n",
    "        return out, adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now try our our layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gcn_layer_83 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "(6, 100)\n",
      "(<tf.Tensor: shape=(6, 100), dtype=float32, numpy=\n",
      "array([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.]], dtype=float32)>, <tf.Tensor: shape=(6, 6), dtype=float32, numpy=\n",
      "array([[1., 1., 1., 1., 1., 0.],\n",
      "       [1., 1., 0., 0., 0., 1.],\n",
      "       [1., 0., 1., 0., 0., 0.],\n",
      "       [1., 0., 0., 1., 0., 0.],\n",
      "       [1., 0., 0., 0., 1., 0.],\n",
      "       [0., 1., 0., 0., 0., 1.]], dtype=float32)>)\n",
      "TensorShape([6, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(6, 100), dtype=float32, numpy=\n",
       " array([[0.02941536, 0.        , 0.        , 0.04078373, 0.        ,\n",
       "         0.07009783, 0.        , 0.01969463, 0.        , 0.06484056,\n",
       "         0.02404069, 0.11943706, 0.        , 0.02303447, 0.        ,\n",
       "         0.        , 0.07462262, 0.        , 0.        , 0.        ,\n",
       "         0.00236331, 0.        , 0.01863228, 0.08786516, 0.10768249,\n",
       "         0.        , 0.05453076, 0.01734958, 0.03943511, 0.        ,\n",
       "         0.        , 0.09943222, 0.        , 0.07867038, 0.        ,\n",
       "         0.        , 0.00390132, 0.        , 0.05327757, 0.08313295,\n",
       "         0.        , 0.07930585, 0.02890965, 0.04306841, 0.        ,\n",
       "         0.        , 0.11955066, 0.02105653, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.00250089, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.00670405, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.02924771, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.03578609, 0.        , 0.        , 0.04215167,\n",
       "         0.0561007 , 0.        , 0.        , 0.0832867 , 0.        ,\n",
       "         0.        , 0.        , 0.09186063, 0.01728042, 0.00788409,\n",
       "         0.        , 0.        , 0.00220828, 0.03819194, 0.        ,\n",
       "         0.        , 0.10586331, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.05886503, 0.02171505, 0.08933407],\n",
       "        [0.02397848, 0.        , 0.        , 0.03180617, 0.00285096,\n",
       "         0.00227307, 0.        , 0.0386356 , 0.        , 0.037289  ,\n",
       "         0.        , 0.11304434, 0.        , 0.        , 0.03512179,\n",
       "         0.        , 0.04447311, 0.        , 0.        , 0.02271605,\n",
       "         0.        , 0.        , 0.04051074, 0.07139541, 0.06838328,\n",
       "         0.        , 0.01424453, 0.        , 0.07670677, 0.        ,\n",
       "         0.        , 0.09171837, 0.        , 0.09034407, 0.        ,\n",
       "         0.        , 0.04997809, 0.        , 0.01654789, 0.03120429,\n",
       "         0.        , 0.07092245, 0.07048462, 0.03268294, 0.        ,\n",
       "         0.        , 0.1248063 , 0.04087969, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.04027573, 0.        , 0.        , 0.02965807,\n",
       "         0.00789624, 0.00616774, 0.        , 0.        , 0.        ,\n",
       "         0.02638557, 0.        , 0.        , 0.02361628, 0.03153595,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.05411819,\n",
       "         0.04375938, 0.        , 0.        , 0.05323562, 0.        ,\n",
       "         0.        , 0.        , 0.06232568, 0.        , 0.        ,\n",
       "         0.        , 0.00414082, 0.        , 0.04593311, 0.        ,\n",
       "         0.        , 0.06796256, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.04906344, 0.0232055 , 0.10323709],\n",
       "        [0.05150314, 0.        , 0.        , 0.08648331, 0.02356013,\n",
       "         0.07044543, 0.        , 0.05495591, 0.        , 0.01101934,\n",
       "         0.00149907, 0.12772664, 0.00381843, 0.03307597, 0.        ,\n",
       "         0.        , 0.08065916, 0.        , 0.        , 0.05167835,\n",
       "         0.        , 0.        , 0.07810642, 0.10005608, 0.07759663,\n",
       "         0.        , 0.        , 0.        , 0.06102264, 0.        ,\n",
       "         0.        , 0.09206979, 0.        , 0.08878153, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.03988973, 0.04632184,\n",
       "         0.        , 0.09583898, 0.05682153, 0.06975477, 0.        ,\n",
       "         0.        , 0.11463907, 0.06523237, 0.        , 0.        ,\n",
       "         0.07441147, 0.        , 0.        , 0.        , 0.07083564,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.07512905, 0.        , 0.06547946, 0.0111058 , 0.        ,\n",
       "         0.        , 0.01930296, 0.        , 0.        , 0.        ,\n",
       "         0.12030437, 0.        , 0.        , 0.12948072, 0.        ,\n",
       "         0.        , 0.        , 0.0394409 , 0.02784908, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.00968813, 0.        ,\n",
       "         0.        , 0.10268918, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.06972355, 0.07490946, 0.10965275],\n",
       "        [0.05150314, 0.        , 0.        , 0.08648331, 0.02356013,\n",
       "         0.07044543, 0.        , 0.05495591, 0.        , 0.01101934,\n",
       "         0.00149907, 0.12772664, 0.00381843, 0.03307597, 0.        ,\n",
       "         0.        , 0.08065916, 0.        , 0.        , 0.05167835,\n",
       "         0.        , 0.        , 0.07810642, 0.10005608, 0.07759663,\n",
       "         0.        , 0.        , 0.        , 0.06102264, 0.        ,\n",
       "         0.        , 0.09206979, 0.        , 0.08878153, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.03988973, 0.04632184,\n",
       "         0.        , 0.09583898, 0.05682153, 0.06975477, 0.        ,\n",
       "         0.        , 0.11463907, 0.06523237, 0.        , 0.        ,\n",
       "         0.07441147, 0.        , 0.        , 0.        , 0.07083564,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.07512905, 0.        , 0.06547946, 0.0111058 , 0.        ,\n",
       "         0.        , 0.01930296, 0.        , 0.        , 0.        ,\n",
       "         0.12030437, 0.        , 0.        , 0.12948072, 0.        ,\n",
       "         0.        , 0.        , 0.0394409 , 0.02784908, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.00968813, 0.        ,\n",
       "         0.        , 0.10268918, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.06972355, 0.07490946, 0.10965275],\n",
       "        [0.05150314, 0.        , 0.        , 0.08648331, 0.02356013,\n",
       "         0.07044543, 0.        , 0.05495591, 0.        , 0.01101934,\n",
       "         0.00149907, 0.12772664, 0.00381843, 0.03307597, 0.        ,\n",
       "         0.        , 0.08065916, 0.        , 0.        , 0.05167835,\n",
       "         0.        , 0.        , 0.07810642, 0.10005608, 0.07759663,\n",
       "         0.        , 0.        , 0.        , 0.06102264, 0.        ,\n",
       "         0.        , 0.09206979, 0.        , 0.08878153, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.03988973, 0.04632184,\n",
       "         0.        , 0.09583898, 0.05682153, 0.06975477, 0.        ,\n",
       "         0.        , 0.11463907, 0.06523237, 0.        , 0.        ,\n",
       "         0.07441147, 0.        , 0.        , 0.        , 0.07083564,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.07512905, 0.        , 0.06547946, 0.0111058 , 0.        ,\n",
       "         0.        , 0.01930296, 0.        , 0.        , 0.        ,\n",
       "         0.12030437, 0.        , 0.        , 0.12948072, 0.        ,\n",
       "         0.        , 0.        , 0.0394409 , 0.02784908, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.00968813, 0.        ,\n",
       "         0.        , 0.10268918, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.06972355, 0.07490946, 0.10965275],\n",
       "        [0.00324992, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.01888166, 0.        , 0.        , 0.        , 0.0979981 ,\n",
       "         0.00343931, 0.10635294, 0.        , 0.        , 0.03179269,\n",
       "         0.        , 0.04597395, 0.        , 0.        , 0.        ,\n",
       "         0.00386982, 0.        , 0.        , 0.06332192, 0.10829395,\n",
       "         0.00604604, 0.08259838, 0.01258365, 0.04580133, 0.        ,\n",
       "         0.        , 0.10100925, 0.        , 0.0773145 , 0.        ,\n",
       "         0.04244602, 0.04488986, 0.02551366, 0.03911815, 0.08099757,\n",
       "         0.        , 0.05648517, 0.03217899, 0.00859293, 0.        ,\n",
       "         0.        , 0.12840398, 0.        , 0.        , 0.01308756,\n",
       "         0.        , 0.        , 0.08077885, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.01511466,\n",
       "         0.08836349, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.00533329, 0.03682646,\n",
       "         0.        , 0.02048756, 0.        , 0.        , 0.09402475,\n",
       "         0.        , 0.        , 0.01673195, 0.01455437, 0.        ,\n",
       "         0.01703791, 0.        , 0.12212914, 0.        , 0.03566428,\n",
       "         0.        , 0.        , 0.03331825, 0.07250161, 0.        ,\n",
       "         0.04312316, 0.08061187, 0.05173425, 0.02222201, 0.        ,\n",
       "         0.        , 0.        , 0.04065531, 0.        , 0.07944266]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(6, 6), dtype=float32, numpy=\n",
       " array([[1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 0., 0., 0., 1.],\n",
       "        [1., 0., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0., 0., 1.]], dtype=float32)>)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcnlayer = GCNLayer('relu')\n",
    "# this step is normally done for us when \n",
    "# using a model, so do not worry about understanding it\n",
    "gcnlayer((nodes, adj_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It outputs (1) the new node features and (2) the adjacency matrix. Let's make sure we can stack these and apply the GCN multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(6, 100), dtype=float32, numpy=\n",
      "array([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.]], dtype=float32)>, <tf.Tensor: shape=(6, 6), dtype=float32, numpy=\n",
      "array([[1., 1., 1., 1., 1., 0.],\n",
      "       [1., 1., 0., 0., 0., 1.],\n",
      "       [1., 0., 1., 0., 0., 0.],\n",
      "       [1., 0., 0., 1., 0., 0.],\n",
      "       [1., 0., 0., 0., 1., 0.],\n",
      "       [0., 1., 0., 0., 0., 1.]], dtype=float32)>)\n",
      "TensorShape([6, 100])\n",
      "(<tf.Tensor: shape=(6, 100), dtype=float32, numpy=\n",
      "array([[0.02941536, 0.        , 0.        , 0.04078373, 0.        ,\n",
      "        0.07009783, 0.        , 0.01969463, 0.        , 0.06484056,\n",
      "        0.02404069, 0.11943706, 0.        , 0.02303447, 0.        ,\n",
      "        0.        , 0.07462262, 0.        , 0.        , 0.        ,\n",
      "        0.00236331, 0.        , 0.01863228, 0.08786516, 0.10768249,\n",
      "        0.        , 0.05453076, 0.01734958, 0.03943511, 0.        ,\n",
      "        0.        , 0.09943222, 0.        , 0.07867038, 0.        ,\n",
      "        0.        , 0.00390132, 0.        , 0.05327757, 0.08313295,\n",
      "        0.        , 0.07930585, 0.02890965, 0.04306841, 0.        ,\n",
      "        0.        , 0.11955066, 0.02105653, 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.00250089, 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.00670405, 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.02924771, 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.03578609, 0.        , 0.        , 0.04215167,\n",
      "        0.0561007 , 0.        , 0.        , 0.0832867 , 0.        ,\n",
      "        0.        , 0.        , 0.09186063, 0.01728042, 0.00788409,\n",
      "        0.        , 0.        , 0.00220828, 0.03819194, 0.        ,\n",
      "        0.        , 0.10586331, 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.05886503, 0.02171505, 0.08933407],\n",
      "       [0.02397848, 0.        , 0.        , 0.03180617, 0.00285096,\n",
      "        0.00227307, 0.        , 0.0386356 , 0.        , 0.037289  ,\n",
      "        0.        , 0.11304434, 0.        , 0.        , 0.03512179,\n",
      "        0.        , 0.04447311, 0.        , 0.        , 0.02271605,\n",
      "        0.        , 0.        , 0.04051074, 0.07139541, 0.06838328,\n",
      "        0.        , 0.01424453, 0.        , 0.07670677, 0.        ,\n",
      "        0.        , 0.09171837, 0.        , 0.09034407, 0.        ,\n",
      "        0.        , 0.04997809, 0.        , 0.01654789, 0.03120429,\n",
      "        0.        , 0.07092245, 0.07048462, 0.03268294, 0.        ,\n",
      "        0.        , 0.1248063 , 0.04087969, 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.04027573, 0.        , 0.        , 0.02965807,\n",
      "        0.00789624, 0.00616774, 0.        , 0.        , 0.        ,\n",
      "        0.02638557, 0.        , 0.        , 0.02361628, 0.03153595,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.05411819,\n",
      "        0.04375938, 0.        , 0.        , 0.05323562, 0.        ,\n",
      "        0.        , 0.        , 0.06232568, 0.        , 0.        ,\n",
      "        0.        , 0.00414082, 0.        , 0.04593311, 0.        ,\n",
      "        0.        , 0.06796256, 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.04906344, 0.0232055 , 0.10323709],\n",
      "       [0.05150314, 0.        , 0.        , 0.08648331, 0.02356013,\n",
      "        0.07044543, 0.        , 0.05495591, 0.        , 0.01101934,\n",
      "        0.00149907, 0.12772664, 0.00381843, 0.03307597, 0.        ,\n",
      "        0.        , 0.08065916, 0.        , 0.        , 0.05167835,\n",
      "        0.        , 0.        , 0.07810642, 0.10005608, 0.07759663,\n",
      "        0.        , 0.        , 0.        , 0.06102264, 0.        ,\n",
      "        0.        , 0.09206979, 0.        , 0.08878153, 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.03988973, 0.04632184,\n",
      "        0.        , 0.09583898, 0.05682153, 0.06975477, 0.        ,\n",
      "        0.        , 0.11463907, 0.06523237, 0.        , 0.        ,\n",
      "        0.07441147, 0.        , 0.        , 0.        , 0.07083564,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.07512905, 0.        , 0.06547946, 0.0111058 , 0.        ,\n",
      "        0.        , 0.01930296, 0.        , 0.        , 0.        ,\n",
      "        0.12030437, 0.        , 0.        , 0.12948072, 0.        ,\n",
      "        0.        , 0.        , 0.0394409 , 0.02784908, 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.00968813, 0.        ,\n",
      "        0.        , 0.10268918, 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.06972355, 0.07490946, 0.10965275],\n",
      "       [0.05150314, 0.        , 0.        , 0.08648331, 0.02356013,\n",
      "        0.07044543, 0.        , 0.05495591, 0.        , 0.01101934,\n",
      "        0.00149907, 0.12772664, 0.00381843, 0.03307597, 0.        ,\n",
      "        0.        , 0.08065916, 0.        , 0.        , 0.05167835,\n",
      "        0.        , 0.        , 0.07810642, 0.10005608, 0.07759663,\n",
      "        0.        , 0.        , 0.        , 0.06102264, 0.        ,\n",
      "        0.        , 0.09206979, 0.        , 0.08878153, 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.03988973, 0.04632184,\n",
      "        0.        , 0.09583898, 0.05682153, 0.06975477, 0.        ,\n",
      "        0.        , 0.11463907, 0.06523237, 0.        , 0.        ,\n",
      "        0.07441147, 0.        , 0.        , 0.        , 0.07083564,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.07512905, 0.        , 0.06547946, 0.0111058 , 0.        ,\n",
      "        0.        , 0.01930296, 0.        , 0.        , 0.        ,\n",
      "        0.12030437, 0.        , 0.        , 0.12948072, 0.        ,\n",
      "        0.        , 0.        , 0.0394409 , 0.02784908, 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.00968813, 0.        ,\n",
      "        0.        , 0.10268918, 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.06972355, 0.07490946, 0.10965275],\n",
      "       [0.05150314, 0.        , 0.        , 0.08648331, 0.02356013,\n",
      "        0.07044543, 0.        , 0.05495591, 0.        , 0.01101934,\n",
      "        0.00149907, 0.12772664, 0.00381843, 0.03307597, 0.        ,\n",
      "        0.        , 0.08065916, 0.        , 0.        , 0.05167835,\n",
      "        0.        , 0.        , 0.07810642, 0.10005608, 0.07759663,\n",
      "        0.        , 0.        , 0.        , 0.06102264, 0.        ,\n",
      "        0.        , 0.09206979, 0.        , 0.08878153, 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.03988973, 0.04632184,\n",
      "        0.        , 0.09583898, 0.05682153, 0.06975477, 0.        ,\n",
      "        0.        , 0.11463907, 0.06523237, 0.        , 0.        ,\n",
      "        0.07441147, 0.        , 0.        , 0.        , 0.07083564,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.07512905, 0.        , 0.06547946, 0.0111058 , 0.        ,\n",
      "        0.        , 0.01930296, 0.        , 0.        , 0.        ,\n",
      "        0.12030437, 0.        , 0.        , 0.12948072, 0.        ,\n",
      "        0.        , 0.        , 0.0394409 , 0.02784908, 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.00968813, 0.        ,\n",
      "        0.        , 0.10268918, 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.06972355, 0.07490946, 0.10965275],\n",
      "       [0.00324992, 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.01888166, 0.        , 0.        , 0.        , 0.0979981 ,\n",
      "        0.00343931, 0.10635294, 0.        , 0.        , 0.03179269,\n",
      "        0.        , 0.04597395, 0.        , 0.        , 0.        ,\n",
      "        0.00386982, 0.        , 0.        , 0.06332192, 0.10829395,\n",
      "        0.00604604, 0.08259838, 0.01258365, 0.04580133, 0.        ,\n",
      "        0.        , 0.10100925, 0.        , 0.0773145 , 0.        ,\n",
      "        0.04244602, 0.04488986, 0.02551366, 0.03911815, 0.08099757,\n",
      "        0.        , 0.05648517, 0.03217899, 0.00859293, 0.        ,\n",
      "        0.        , 0.12840398, 0.        , 0.        , 0.01308756,\n",
      "        0.        , 0.        , 0.08077885, 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.01511466,\n",
      "        0.08836349, 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.00533329, 0.03682646,\n",
      "        0.        , 0.02048756, 0.        , 0.        , 0.09402475,\n",
      "        0.        , 0.        , 0.01673195, 0.01455437, 0.        ,\n",
      "        0.01703791, 0.        , 0.12212914, 0.        , 0.03566428,\n",
      "        0.        , 0.        , 0.03331825, 0.07250161, 0.        ,\n",
      "        0.04312316, 0.08061187, 0.05173425, 0.02222201, 0.        ,\n",
      "        0.        , 0.        , 0.04065531, 0.        , 0.07944266]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(6, 6), dtype=float32, numpy=\n",
      "array([[1., 1., 1., 1., 1., 0.],\n",
      "       [1., 1., 0., 0., 0., 1.],\n",
      "       [1., 0., 1., 0., 0., 0.],\n",
      "       [1., 0., 0., 1., 0., 0.],\n",
      "       [1., 0., 0., 0., 1., 0.],\n",
      "       [0., 1., 0., 0., 0., 1.]], dtype=float32)>)\n",
      "TensorShape([6, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(6, 100), dtype=float32, numpy=\n",
       " array([[0.        , 0.        , 0.        , 0.        , 0.0374127 ,\n",
       "         0.04892833, 0.05149623, 0.04459392, 0.05778711, 0.        ,\n",
       "         0.        , 0.01876557, 0.02093326, 0.04393142, 0.        ,\n",
       "         0.01315187, 0.01300867, 0.        , 0.00547662, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.00770175,\n",
       "         0.        , 0.00897225, 0.        , 0.        , 0.05867001,\n",
       "         0.        , 0.00052927, 0.        , 0.        , 0.08040065,\n",
       "         0.        , 0.        , 0.        , 0.0553322 , 0.        ,\n",
       "         0.        , 0.02552522, 0.        , 0.        , 0.04558952,\n",
       "         0.        , 0.06765645, 0.03710802, 0.0726962 , 0.        ,\n",
       "         0.        , 0.        , 0.02830723, 0.0650677 , 0.        ,\n",
       "         0.        , 0.        , 0.05519677, 0.        , 0.02332223,\n",
       "         0.        , 0.08099704, 0.        , 0.        , 0.02391953,\n",
       "         0.01238158, 0.05003433, 0.00980989, 0.        , 0.05571296,\n",
       "         0.04917232, 0.04031105, 0.        , 0.01607707, 0.02776548,\n",
       "         0.        , 0.        , 0.00424984, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.04615152,\n",
       "         0.        , 0.        , 0.07134067, 0.        , 0.02489531,\n",
       "         0.03001106, 0.05579953, 0.04272125, 0.        , 0.02662767,\n",
       "         0.04318321, 0.03958971, 0.05395536, 0.        , 0.05155755],\n",
       "        [0.        , 0.        , 0.        , 0.022647  , 0.03919133,\n",
       "         0.05053455, 0.03244845, 0.05100929, 0.02524355, 0.        ,\n",
       "         0.        , 0.03213933, 0.04560682, 0.02536859, 0.01640656,\n",
       "         0.01139102, 0.03480309, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.00217603, 0.        , 0.01337979, 0.01148013,\n",
       "         0.        , 0.03007665, 0.00151242, 0.        , 0.04681329,\n",
       "         0.00241809, 0.0151812 , 0.        , 0.        , 0.0686658 ,\n",
       "         0.00382565, 0.        , 0.        , 0.01565873, 0.        ,\n",
       "         0.        , 0.00842205, 0.        , 0.        , 0.04513453,\n",
       "         0.        , 0.04157133, 0.04183352, 0.04198755, 0.01051311,\n",
       "         0.01534525, 0.        , 0.0283329 , 0.05627758, 0.        ,\n",
       "         0.        , 0.        , 0.04472226, 0.        , 0.03529242,\n",
       "         0.        , 0.04680091, 0.        , 0.00571799, 0.02933199,\n",
       "         0.02241511, 0.07283634, 0.01801289, 0.00521124, 0.0363611 ,\n",
       "         0.05949184, 0.02490122, 0.        , 0.02178704, 0.01478952,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.00424285,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.02150968,\n",
       "         0.        , 0.        , 0.04856372, 0.        , 0.01287779,\n",
       "         0.03429808, 0.03501878, 0.03864636, 0.        , 0.00188879,\n",
       "         0.02032029, 0.03515609, 0.06206221, 0.        , 0.04325807],\n",
       "        [0.        , 0.        , 0.        , 0.00632199, 0.03383501,\n",
       "         0.04517863, 0.04811963, 0.04669585, 0.05875301, 0.        ,\n",
       "         0.        , 0.02270955, 0.0301566 , 0.0426348 , 0.01077481,\n",
       "         0.0147533 , 0.01379443, 0.        , 0.01255883, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.00304296, 0.00818776,\n",
       "         0.        , 0.01008521, 0.        , 0.        , 0.06126598,\n",
       "         0.        , 0.00403162, 0.        , 0.        , 0.08884779,\n",
       "         0.        , 0.        , 0.        , 0.04680403, 0.        ,\n",
       "         0.        , 0.01884959, 0.        , 0.        , 0.04648124,\n",
       "         0.        , 0.07199182, 0.03509724, 0.07438262, 0.        ,\n",
       "         0.        , 0.        , 0.03057127, 0.06288654, 0.        ,\n",
       "         0.        , 0.        , 0.04745901, 0.        , 0.02066624,\n",
       "         0.        , 0.08101463, 0.        , 0.        , 0.02545316,\n",
       "         0.014215  , 0.05761097, 0.01023572, 0.        , 0.05980258,\n",
       "         0.04808521, 0.04746361, 0.        , 0.02358134, 0.02095506,\n",
       "         0.        , 0.        , 0.00052435, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.04098343,\n",
       "         0.        , 0.        , 0.05949353, 0.        , 0.02229141,\n",
       "         0.02821092, 0.04855073, 0.04358017, 0.        , 0.02756788,\n",
       "         0.03591783, 0.03805162, 0.05988389, 0.00030968, 0.04386716],\n",
       "        [0.        , 0.        , 0.        , 0.00632199, 0.03383501,\n",
       "         0.04517863, 0.04811963, 0.04669585, 0.05875301, 0.        ,\n",
       "         0.        , 0.02270955, 0.0301566 , 0.0426348 , 0.01077481,\n",
       "         0.0147533 , 0.01379443, 0.        , 0.01255883, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.00304296, 0.00818776,\n",
       "         0.        , 0.01008521, 0.        , 0.        , 0.06126598,\n",
       "         0.        , 0.00403162, 0.        , 0.        , 0.08884779,\n",
       "         0.        , 0.        , 0.        , 0.04680403, 0.        ,\n",
       "         0.        , 0.01884959, 0.        , 0.        , 0.04648124,\n",
       "         0.        , 0.07199182, 0.03509724, 0.07438262, 0.        ,\n",
       "         0.        , 0.        , 0.03057127, 0.06288654, 0.        ,\n",
       "         0.        , 0.        , 0.04745901, 0.        , 0.02066624,\n",
       "         0.        , 0.08101463, 0.        , 0.        , 0.02545316,\n",
       "         0.014215  , 0.05761097, 0.01023572, 0.        , 0.05980258,\n",
       "         0.04808521, 0.04746361, 0.        , 0.02358134, 0.02095506,\n",
       "         0.        , 0.        , 0.00052435, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.04098343,\n",
       "         0.        , 0.        , 0.05949353, 0.        , 0.02229141,\n",
       "         0.02821092, 0.04855073, 0.04358017, 0.        , 0.02756788,\n",
       "         0.03591783, 0.03805162, 0.05988389, 0.00030968, 0.04386716],\n",
       "        [0.        , 0.        , 0.        , 0.00632199, 0.03383501,\n",
       "         0.04517863, 0.04811963, 0.04669585, 0.05875301, 0.        ,\n",
       "         0.        , 0.02270955, 0.0301566 , 0.0426348 , 0.01077481,\n",
       "         0.0147533 , 0.01379443, 0.        , 0.01255883, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.00304296, 0.00818776,\n",
       "         0.        , 0.01008521, 0.        , 0.        , 0.06126598,\n",
       "         0.        , 0.00403162, 0.        , 0.        , 0.08884779,\n",
       "         0.        , 0.        , 0.        , 0.04680403, 0.        ,\n",
       "         0.        , 0.01884959, 0.        , 0.        , 0.04648124,\n",
       "         0.        , 0.07199182, 0.03509724, 0.07438262, 0.        ,\n",
       "         0.        , 0.        , 0.03057127, 0.06288654, 0.        ,\n",
       "         0.        , 0.        , 0.04745901, 0.        , 0.02066624,\n",
       "         0.        , 0.08101463, 0.        , 0.        , 0.02545316,\n",
       "         0.014215  , 0.05761097, 0.01023572, 0.        , 0.05980258,\n",
       "         0.04808521, 0.04746361, 0.        , 0.02358134, 0.02095506,\n",
       "         0.        , 0.        , 0.00052435, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.04098343,\n",
       "         0.        , 0.        , 0.05949353, 0.        , 0.02229141,\n",
       "         0.02821092, 0.04855073, 0.04358017, 0.        , 0.02756788,\n",
       "         0.03591783, 0.03805162, 0.05988389, 0.00030968, 0.04386716],\n",
       "        [0.        , 0.        , 0.        , 0.02216214, 0.04395132,\n",
       "         0.05629404, 0.0310896 , 0.05286903, 0.01640691, 0.        ,\n",
       "         0.        , 0.03171421, 0.04479062, 0.021047  , 0.00864165,\n",
       "         0.00952521, 0.04265391, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.00318523, 0.        , 0.01464507, 0.01090313,\n",
       "         0.        , 0.03114476, 0.00842995, 0.        , 0.04379308,\n",
       "         0.00046049, 0.01667977, 0.        , 0.        , 0.05723943,\n",
       "         0.00926039, 0.        , 0.        , 0.0129791 , 0.        ,\n",
       "         0.        , 0.01033514, 0.        , 0.        , 0.04409203,\n",
       "         0.        , 0.02981336, 0.04610913, 0.03109357, 0.01994869,\n",
       "         0.02433263, 0.00306258, 0.02517848, 0.05703115, 0.        ,\n",
       "         0.        , 0.        , 0.05155763, 0.        , 0.04231373,\n",
       "         0.        , 0.03835629, 0.        , 0.01231855, 0.02782087,\n",
       "         0.02312382, 0.07235403, 0.01886696, 0.01067862, 0.02593164,\n",
       "         0.06647776, 0.01474336, 0.        , 0.01497712, 0.01512816,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.00350253,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.01997779,\n",
       "         0.        , 0.        , 0.05470198, 0.        , 0.011851  ,\n",
       "         0.0370722 , 0.03601316, 0.03666959, 0.        , 0.        ,\n",
       "         0.02009055, 0.03659488, 0.05730088, 0.        , 0.05133199]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(6, 6), dtype=float32, numpy=\n",
       " array([[1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 0., 0., 0., 1.],\n",
       "        [1., 0., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0., 0., 1.]], dtype=float32)>)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = (nodes, adj_mat)\n",
    "for i in range(2):\n",
    "    x = gcnlayer(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works! Why do we see zeros though? Probably because we had negative numbers that were removed by our ReLU activation. This will be solved by training and increasing our dimension number. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solubility Example\n",
    "\n",
    "We'll now revisit predicting solubility with GCNs. Remember before that we used the features included with the dataset. Now, however we can use the molecular structures directly. Our GCN layer outputs node-level features. To predict solubility, we need to get a graph-level feature. We'll see later how to be more sophisticated in this process, but for now let's just take the average over all node features after our GCN layers. This is simple, permutation invariant, and gets us from node-level to graph level. Here's an implementation of this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRLayer(tf.keras.layers.Layer):\n",
    "    '''A GNN layer that computes average over all node features'''\n",
    "    def __init__(self, name='GRLayer', **kwargs):\n",
    "        super(GRLayer, self).__init__(name=name, **kwargs)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        nodes, adj = inputs\n",
    "        reduction = tf.reduce_mean(nodes, axis=0)\n",
    "        # make batch size be 1 because we're receiving one molecule\n",
    "        return tf.reshape(reduction, (1, -1))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete our deep solubility predictor, we can add some dense layers and make sure we have a single-output without activation since we're doing regression. Note this model is defined using the [Keras functional API](https://keras.io/guides/functional_api/) which is necessary when you have multiple inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 100)\n",
      "[<tf.Tensor 'input_53:0' shape=(None, 100) dtype=float32>, <tf.Tensor 'input_54:0' shape=(None, None) dtype=float32>]\n",
      "(None, 100)\n",
      "(<tf.Tensor 'gcn_layer_88/Relu:0' shape=(None, 100) dtype=float32>, <tf.Tensor 'gcn_layer_88/gcn_layer_88/Identity:0' shape=(None, None) dtype=float32>)\n",
      "(None, 100)\n",
      "(<tf.Tensor 'gcn_layer_89/Relu:0' shape=(None, 100) dtype=float32>, <tf.Tensor 'gcn_layer_89/gcn_layer_89/Identity:0' shape=(None, None) dtype=float32>)\n",
      "(None, 100)\n",
      "(<tf.Tensor 'gcn_layer_90/Relu:0' shape=(None, 100) dtype=float32>, <tf.Tensor 'gcn_layer_90/gcn_layer_90/Identity:0' shape=(None, None) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "ninput = tf.keras.Input((100,))\n",
    "ainput = tf.keras.Input((None,))\n",
    "# GCN block\n",
    "x = GCNLayer('relu')([ninput, ainput])\n",
    "x = GCNLayer('relu')(x)\n",
    "x = GCNLayer('relu')(x)\n",
    "x = GCNLayer('relu')(x)\n",
    "# reduce to graph features\n",
    "x = GRLayer()(x)\n",
    "# standard layers\n",
    "x = tf.keras.layers.Dense(16, 'tanh')(x)\n",
    "x = tf.keras.layers.Dense(1)(x)\n",
    "model = tf.keras.Model(inputs=(ninput, ainput), outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where does the 100 come from? Well, this dataset has lots of elements so we cannout use our size 3 one-hot encondings because we'll have more than 3 unique elements. We previously only had C, H and O. This is a good time to updaet our `smiles2graph` function to deal with this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "tags": [
     "hidden-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def gen_smiles2graph(sml):\n",
    "    '''Argument for the RD2NX function should be a valid SMILES sequence\n",
    "    returns: the graph\n",
    "    '''\n",
    "    m = rdkit.Chem.MolFromSmiles(sml)\n",
    "    m = rdkit.Chem.AddHs(m)\n",
    "    order_string = {rdkit.Chem.rdchem.BondType.SINGLE: 1,\n",
    "                    rdkit.Chem.rdchem.BondType.DOUBLE: 2,\n",
    "                    rdkit.Chem.rdchem.BondType.TRIPLE: 3,\n",
    "                    rdkit.Chem.rdchem.BondType.AROMATIC: 4}\n",
    "    N = len(list(m.GetAtoms()))\n",
    "    nodes = np.zeros((N,100))\n",
    "    for i in m.GetAtoms():\n",
    "        nodes[i.GetIdx(), i.GetAtomicNum()] = 1\n",
    "    \n",
    "    adj = np.zeros((N,N))\n",
    "    for j in m.GetBonds():\n",
    "        u = min(j.GetBeginAtomIdx(),j.GetEndAtomIdx())\n",
    "        v = max(j.GetBeginAtomIdx(),j.GetEndAtomIdx())        \n",
    "        order = j.GetBondType()\n",
    "        if order in order_string:\n",
    "            order = order_string[order]\n",
    "        else:\n",
    "            raise Warning('Ignoring bond order' + order)\n",
    "        adj[u, v] = 1        \n",
    "        adj[v, u] = 1\n",
    "    adj += np.eye(N)\n",
    "    return nodes, adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(6, 100), dtype=float32, numpy=\n",
      "array([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.]], dtype=float32)>, <tf.Tensor: shape=(6, 6), dtype=float32, numpy=\n",
      "array([[1., 1., 1., 1., 1., 0.],\n",
      "       [1., 1., 0., 0., 0., 1.],\n",
      "       [1., 0., 1., 0., 0., 0.],\n",
      "       [1., 0., 0., 1., 0., 0.],\n",
      "       [1., 0., 0., 0., 1., 0.],\n",
      "       [0., 1., 0., 0., 0., 1.]], dtype=float32)>]\n",
      "TensorShape([6, 100])\n",
      "(<tf.Tensor: shape=(6, 100), dtype=float32, numpy=\n",
      "array([[0.        , 0.        , 0.05176611, 0.        , 0.0424268 ,\n",
      "        0.        , 0.        , 0.01648513, 0.        , 0.        ,\n",
      "        0.        , 0.01928929, 0.05921141, 0.        , 0.        ,\n",
      "        0.12034347, 0.05466471, 0.02194615, 0.        , 0.0384324 ,\n",
      "        0.        , 0.03856621, 0.10922831, 0.        , 0.        ,\n",
      "        0.02383926, 0.        , 0.        , 0.03319268, 0.        ,\n",
      "        0.        , 0.        , 0.10197387, 0.        , 0.        ,\n",
      "        0.        , 0.04542685, 0.09236013, 0.        , 0.00392079,\n",
      "        0.02907018, 0.04339688, 0.03353558, 0.0467534 , 0.07427587,\n",
      "        0.        , 0.05327306, 0.        , 0.        , 0.        ,\n",
      "        0.12801333, 0.00290193, 0.        , 0.04800035, 0.        ,\n",
      "        0.01005514, 0.        , 0.01574862, 0.0051035 , 0.        ,\n",
      "        0.        , 0.05731497, 0.0733973 , 0.        , 0.12017145,\n",
      "        0.        , 0.10677297, 0.04567691, 0.01618365, 0.02907376,\n",
      "        0.0461513 , 0.        , 0.        , 0.        , 0.06841906,\n",
      "        0.        , 0.02124773, 0.        , 0.        , 0.04886484,\n",
      "        0.03069441, 0.06689598, 0.        , 0.        , 0.09252912,\n",
      "        0.        , 0.07492816, 0.        , 0.09155913, 0.12665805,\n",
      "        0.        , 0.03158448, 0.        , 0.        , 0.08960336,\n",
      "        0.0119982 , 0.1163068 , 0.        , 0.03374182, 0.09970988],\n",
      "       [0.        , 0.        , 0.07582729, 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.02163588, 0.01913297, 0.        ,\n",
      "        0.        , 0.02215653, 0.02718852, 0.        , 0.00539565,\n",
      "        0.1005105 , 0.07469702, 0.        , 0.        , 0.0535776 ,\n",
      "        0.        , 0.04844655, 0.08691227, 0.        , 0.        ,\n",
      "        0.02132232, 0.        , 0.        , 0.01475812, 0.01472151,\n",
      "        0.        , 0.        , 0.09650775, 0.04635089, 0.        ,\n",
      "        0.        , 0.01302388, 0.10994932, 0.01136814, 0.03682485,\n",
      "        0.05133943, 0.        , 0.05829093, 0.05215395, 0.10813504,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.00479706,\n",
      "        0.10080048, 0.        , 0.02639542, 0.00426724, 0.        ,\n",
      "        0.        , 0.        , 0.00253987, 0.        , 0.        ,\n",
      "        0.00187975, 0.04571868, 0.0866667 , 0.        , 0.10004516,\n",
      "        0.        , 0.08546461, 0.        , 0.        , 0.03613565,\n",
      "        0.02318884, 0.        , 0.        , 0.        , 0.04578777,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.01058138, 0.03358577, 0.        , 0.02732715, 0.0540056 ,\n",
      "        0.        , 0.10294281, 0.        , 0.05489006, 0.09634078,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.06763318,\n",
      "        0.        , 0.09696534, 0.        , 0.03140737, 0.11156373],\n",
      "       [0.        , 0.        , 0.0455127 , 0.        , 0.0199447 ,\n",
      "        0.        , 0.        , 0.        , 0.00774753, 0.        ,\n",
      "        0.        , 0.        , 0.0508776 , 0.        , 0.00360985,\n",
      "        0.13100052, 0.05353083, 0.        , 0.        , 0.05848398,\n",
      "        0.        , 0.09362081, 0.14205542, 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.00433826, 0.        , 0.1388455 , 0.01018063, 0.        ,\n",
      "        0.        , 0.08832218, 0.11373557, 0.        , 0.00393502,\n",
      "        0.01322576, 0.01707026, 0.08201176, 0.04603881, 0.09286566,\n",
      "        0.        , 0.04069362, 0.        , 0.        , 0.        ,\n",
      "        0.08577868, 0.        , 0.00325561, 0.        , 0.        ,\n",
      "        0.        , 0.01971399, 0.04823143, 0.062928  , 0.        ,\n",
      "        0.        , 0.        , 0.06500643, 0.        , 0.12447393,\n",
      "        0.        , 0.07719997, 0.07333823, 0.        , 0.05974752,\n",
      "        0.        , 0.03150158, 0.        , 0.        , 0.00081158,\n",
      "        0.        , 0.03774045, 0.        , 0.        , 0.00486407,\n",
      "        0.05372389, 0.03238957, 0.        , 0.        , 0.15669788,\n",
      "        0.        , 0.10050423, 0.        , 0.12508884, 0.08377453,\n",
      "        0.        , 0.0328995 , 0.        , 0.        , 0.03141694,\n",
      "        0.        , 0.09016714, 0.        , 0.        , 0.11628534],\n",
      "       [0.        , 0.        , 0.0455127 , 0.        , 0.0199447 ,\n",
      "        0.        , 0.        , 0.        , 0.00774753, 0.        ,\n",
      "        0.        , 0.        , 0.0508776 , 0.        , 0.00360985,\n",
      "        0.13100052, 0.05353083, 0.        , 0.        , 0.05848398,\n",
      "        0.        , 0.09362081, 0.14205542, 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.00433826, 0.        , 0.1388455 , 0.01018063, 0.        ,\n",
      "        0.        , 0.08832218, 0.11373557, 0.        , 0.00393502,\n",
      "        0.01322576, 0.01707026, 0.08201176, 0.04603881, 0.09286566,\n",
      "        0.        , 0.04069362, 0.        , 0.        , 0.        ,\n",
      "        0.08577868, 0.        , 0.00325561, 0.        , 0.        ,\n",
      "        0.        , 0.01971399, 0.04823143, 0.062928  , 0.        ,\n",
      "        0.        , 0.        , 0.06500643, 0.        , 0.12447393,\n",
      "        0.        , 0.07719997, 0.07333823, 0.        , 0.05974752,\n",
      "        0.        , 0.03150158, 0.        , 0.        , 0.00081158,\n",
      "        0.        , 0.03774045, 0.        , 0.        , 0.00486407,\n",
      "        0.05372389, 0.03238957, 0.        , 0.        , 0.15669788,\n",
      "        0.        , 0.10050423, 0.        , 0.12508884, 0.08377453,\n",
      "        0.        , 0.0328995 , 0.        , 0.        , 0.03141694,\n",
      "        0.        , 0.09016714, 0.        , 0.        , 0.11628534],\n",
      "       [0.        , 0.        , 0.0455127 , 0.        , 0.0199447 ,\n",
      "        0.        , 0.        , 0.        , 0.00774753, 0.        ,\n",
      "        0.        , 0.        , 0.0508776 , 0.        , 0.00360985,\n",
      "        0.13100052, 0.05353083, 0.        , 0.        , 0.05848398,\n",
      "        0.        , 0.09362081, 0.14205542, 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.00433826, 0.        , 0.1388455 , 0.01018063, 0.        ,\n",
      "        0.        , 0.08832218, 0.11373557, 0.        , 0.00393502,\n",
      "        0.01322576, 0.01707026, 0.08201176, 0.04603881, 0.09286566,\n",
      "        0.        , 0.04069362, 0.        , 0.        , 0.        ,\n",
      "        0.08577868, 0.        , 0.00325561, 0.        , 0.        ,\n",
      "        0.        , 0.01971399, 0.04823143, 0.062928  , 0.        ,\n",
      "        0.        , 0.        , 0.06500643, 0.        , 0.12447393,\n",
      "        0.        , 0.07719997, 0.07333823, 0.        , 0.05974752,\n",
      "        0.        , 0.03150158, 0.        , 0.        , 0.00081158,\n",
      "        0.        , 0.03774045, 0.        , 0.        , 0.00486407,\n",
      "        0.05372389, 0.03238957, 0.        , 0.        , 0.15669788,\n",
      "        0.        , 0.10050423, 0.        , 0.12508884, 0.08377453,\n",
      "        0.        , 0.0328995 , 0.        , 0.        , 0.03141694,\n",
      "        0.        , 0.09016714, 0.        , 0.        , 0.11628534],\n",
      "       [0.        , 0.        , 0.0760654 , 0.        , 0.01850428,\n",
      "        0.        , 0.        , 0.08343875, 0.00243733, 0.01319744,\n",
      "        0.        , 0.06049281, 0.04352805, 0.        , 0.        ,\n",
      "        0.0948117 , 0.07082281, 0.0004199 , 0.        , 0.02973972,\n",
      "        0.00826871, 0.        , 0.05966418, 0.        , 0.        ,\n",
      "        0.06455372, 0.        , 0.        , 0.06504788, 0.00160117,\n",
      "        0.        , 0.        , 0.06100263, 0.01493397, 0.        ,\n",
      "        0.        , 0.        , 0.08417659, 0.        , 0.02858461,\n",
      "        0.06161653, 0.02405978, 0.00362591, 0.0515184 , 0.08108045,\n",
      "        0.        , 0.0258487 , 0.        , 0.        , 0.        ,\n",
      "        0.14983833, 0.00463853, 0.        , 0.06825291, 0.        ,\n",
      "        0.01493957, 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.11460181, 0.09174021, 0.        , 0.10077424,\n",
      "        0.03361049, 0.1203647 , 0.        , 0.        , 0.00369643,\n",
      "        0.1162658 , 0.        , 0.        , 0.        , 0.11905308,\n",
      "        0.00484664, 0.        , 0.        , 0.03021429, 0.05104535,\n",
      "        0.        , 0.07641974, 0.        , 0.05513673, 0.        ,\n",
      "        0.        , 0.07036307, 0.        , 0.03052762, 0.14680362,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.13131215,\n",
      "        0.01091584, 0.12794037, 0.        , 0.10455776, 0.0920248 ]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(6, 6), dtype=float32, numpy=\n",
      "array([[1., 1., 1., 1., 1., 0.],\n",
      "       [1., 1., 0., 0., 0., 1.],\n",
      "       [1., 0., 1., 0., 0., 0.],\n",
      "       [1., 0., 0., 1., 0., 0.],\n",
      "       [1., 0., 0., 0., 1., 0.],\n",
      "       [0., 1., 0., 0., 0., 1.]], dtype=float32)>)\n",
      "TensorShape([6, 100])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(6, 100), dtype=float32, numpy=\n",
      "array([[0.01681605, 0.        , 0.04823257, 0.        , 0.        ,\n",
      "        0.        , 0.0352641 , 0.02108229, 0.00813875, 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.0549577 , 0.        , 0.04166056, 0.03553317,\n",
      "        0.03208452, 0.04779641, 0.00705159, 0.09495949, 0.        ,\n",
      "        0.05440381, 0.00015609, 0.01865607, 0.        , 0.        ,\n",
      "        0.        , 0.02475368, 0.        , 0.        , 0.09954035,\n",
      "        0.03014004, 0.        , 0.11534689, 0.        , 0.        ,\n",
      "        0.09192654, 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.00233381,\n",
      "        0.        , 0.02209008, 0.        , 0.        , 0.02767377,\n",
      "        0.        , 0.        , 0.01510315, 0.        , 0.        ,\n",
      "        0.03881584, 0.01605416, 0.        , 0.        , 0.06631889,\n",
      "        0.        , 0.03975217, 0.05538071, 0.04349674, 0.        ,\n",
      "        0.        , 0.        , 0.01180344, 0.03435455, 0.        ,\n",
      "        0.01336255, 0.03799669, 0.01027706, 0.        , 0.        ,\n",
      "        0.05749967, 0.02308833, 0.        , 0.03202878, 0.04571117,\n",
      "        0.00656666, 0.03431111, 0.        , 0.        , 0.        ,\n",
      "        0.06666567, 0.        , 0.00051278, 0.01662452, 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.00333585, 0.        ],\n",
      "       [0.00199381, 0.        , 0.03290081, 0.03792508, 0.        ,\n",
      "        0.01695663, 0.03243756, 0.00195986, 0.        , 0.        ,\n",
      "        0.00589892, 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.06388627, 0.        , 0.0275573 , 0.01295435,\n",
      "        0.01623339, 0.01999587, 0.        , 0.07931475, 0.        ,\n",
      "        0.06656976, 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.05125185, 0.        , 0.00482318, 0.07832031,\n",
      "        0.        , 0.        , 0.10729289, 0.01288889, 0.        ,\n",
      "        0.05065024, 0.        , 0.0004074 , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.03123475, 0.        , 0.00569108,\n",
      "        0.        , 0.        , 0.01570944, 0.        , 0.03292137,\n",
      "        0.        , 0.00260146, 0.        , 0.        , 0.        ,\n",
      "        0.03326426, 0.03333253, 0.        , 0.        , 0.0323161 ,\n",
      "        0.        , 0.02587025, 0.05839419, 0.02169864, 0.        ,\n",
      "        0.        , 0.        , 0.040992  , 0.0399397 , 0.        ,\n",
      "        0.        , 0.01588152, 0.0313495 , 0.        , 0.        ,\n",
      "        0.03932922, 0.01574223, 0.        , 0.01693413, 0.05419791,\n",
      "        0.        , 0.03393662, 0.01525447, 0.007147  , 0.        ,\n",
      "        0.05695587, 0.        , 0.02322869, 0.04124898, 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.00554685, 0.        , 0.05248566, 0.        , 0.        ,\n",
      "        0.        , 0.03586087, 0.02317748, 0.00637988, 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.05859273, 0.        , 0.0341772 , 0.04123843,\n",
      "        0.02727682, 0.04714286, 0.        , 0.09479234, 0.        ,\n",
      "        0.06684309, 0.00510417, 0.01435245, 0.        , 0.        ,\n",
      "        0.        , 0.03168872, 0.        , 0.        , 0.09837665,\n",
      "        0.02137617, 0.        , 0.11800353, 0.        , 0.        ,\n",
      "        0.08560562, 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.00617066,\n",
      "        0.        , 0.02116337, 0.        , 0.        , 0.03217272,\n",
      "        0.        , 0.        , 0.01221924, 0.        , 0.        ,\n",
      "        0.03451034, 0.01216073, 0.        , 0.        , 0.06488365,\n",
      "        0.        , 0.03883176, 0.05837122, 0.04379385, 0.        ,\n",
      "        0.        , 0.        , 0.01857368, 0.03166249, 0.        ,\n",
      "        0.00930515, 0.03554623, 0.01572869, 0.        , 0.        ,\n",
      "        0.05339831, 0.01927909, 0.        , 0.03005721, 0.04658009,\n",
      "        0.00308014, 0.04154681, 0.        , 0.        , 0.        ,\n",
      "        0.07636341, 0.        , 0.00369078, 0.01978098, 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.00345048, 0.        ],\n",
      "       [0.00554685, 0.        , 0.05248566, 0.        , 0.        ,\n",
      "        0.        , 0.03586087, 0.02317748, 0.00637988, 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.05859273, 0.        , 0.0341772 , 0.04123843,\n",
      "        0.02727682, 0.04714286, 0.        , 0.09479234, 0.        ,\n",
      "        0.06684309, 0.00510417, 0.01435245, 0.        , 0.        ,\n",
      "        0.        , 0.03168872, 0.        , 0.        , 0.09837665,\n",
      "        0.02137617, 0.        , 0.11800353, 0.        , 0.        ,\n",
      "        0.08560562, 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.00617066,\n",
      "        0.        , 0.02116337, 0.        , 0.        , 0.03217272,\n",
      "        0.        , 0.        , 0.01221924, 0.        , 0.        ,\n",
      "        0.03451034, 0.01216073, 0.        , 0.        , 0.06488365,\n",
      "        0.        , 0.03883176, 0.05837122, 0.04379385, 0.        ,\n",
      "        0.        , 0.        , 0.01857368, 0.03166249, 0.        ,\n",
      "        0.00930515, 0.03554623, 0.01572869, 0.        , 0.        ,\n",
      "        0.05339831, 0.01927909, 0.        , 0.03005721, 0.04658009,\n",
      "        0.00308014, 0.04154681, 0.        , 0.        , 0.        ,\n",
      "        0.07636341, 0.        , 0.00369078, 0.01978098, 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.00345048, 0.        ],\n",
      "       [0.00554685, 0.        , 0.05248566, 0.        , 0.        ,\n",
      "        0.        , 0.03586087, 0.02317748, 0.00637988, 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.05859273, 0.        , 0.0341772 , 0.04123843,\n",
      "        0.02727682, 0.04714286, 0.        , 0.09479234, 0.        ,\n",
      "        0.06684309, 0.00510417, 0.01435245, 0.        , 0.        ,\n",
      "        0.        , 0.03168872, 0.        , 0.        , 0.09837665,\n",
      "        0.02137617, 0.        , 0.11800353, 0.        , 0.        ,\n",
      "        0.08560562, 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.00617066,\n",
      "        0.        , 0.02116337, 0.        , 0.        , 0.03217272,\n",
      "        0.        , 0.        , 0.01221924, 0.        , 0.        ,\n",
      "        0.03451034, 0.01216073, 0.        , 0.        , 0.06488365,\n",
      "        0.        , 0.03883176, 0.05837122, 0.04379385, 0.        ,\n",
      "        0.        , 0.        , 0.01857368, 0.03166249, 0.        ,\n",
      "        0.00930515, 0.03554623, 0.01572869, 0.        , 0.        ,\n",
      "        0.05339831, 0.01927909, 0.        , 0.03005721, 0.04658009,\n",
      "        0.00308014, 0.04154681, 0.        , 0.        , 0.        ,\n",
      "        0.07636341, 0.        , 0.00369078, 0.01978098, 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.00345048, 0.        ],\n",
      "       [0.00833461, 0.        , 0.02491228, 0.04831795, 0.        ,\n",
      "        0.03211629, 0.03067052, 0.        , 0.        , 0.        ,\n",
      "        0.01173034, 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.06510132, 0.        , 0.03017811, 0.        ,\n",
      "        0.01572897, 0.01216416, 0.        , 0.07464387, 0.00576892,\n",
      "        0.05871114, 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.05142508, 0.        , 0.01544795, 0.07488328,\n",
      "        0.        , 0.        , 0.10183734, 0.02024326, 0.        ,\n",
      "        0.04536583, 0.        , 0.01182775, 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.0457265 , 0.        , 0.00193766,\n",
      "        0.        , 0.        , 0.01655615, 0.        , 0.02918587,\n",
      "        0.        , 0.01247257, 0.        , 0.        , 0.        ,\n",
      "        0.03459642, 0.0420739 , 0.        , 0.        , 0.02574493,\n",
      "        0.        , 0.02314414, 0.05638564, 0.01629423, 0.        ,\n",
      "        0.        , 0.        , 0.04128864, 0.04497593, 0.        ,\n",
      "        0.        , 0.01289721, 0.03245157, 0.        , 0.        ,\n",
      "        0.03826467, 0.01762761, 0.        , 0.01426097, 0.05736607,\n",
      "        0.        , 0.02743268, 0.02820055, 0.0088232 , 0.        ,\n",
      "        0.04655872, 0.        , 0.02823568, 0.04422721, 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(6, 6), dtype=float32, numpy=\n",
      "array([[1., 1., 1., 1., 1., 0.],\n",
      "       [1., 1., 0., 0., 0., 1.],\n",
      "       [1., 0., 1., 0., 0., 0.],\n",
      "       [1., 0., 0., 1., 0., 0.],\n",
      "       [1., 0., 0., 0., 1., 0.],\n",
      "       [0., 1., 0., 0., 0., 1.]], dtype=float32)>)\n",
      "TensorShape([6, 100])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(6, 100), dtype=float32, numpy=\n",
      "array([[3.06379888e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "        9.94184241e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "        3.12208515e-02, 2.51045413e-02, 2.24592388e-02, 1.19241746e-02,\n",
      "        0.00000000e+00, 0.00000000e+00, 2.16732118e-02, 0.00000000e+00,\n",
      "        3.49257588e-02, 0.00000000e+00, 3.26467454e-02, 0.00000000e+00,\n",
      "        3.11847217e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "        4.26548626e-03, 0.00000000e+00, 0.00000000e+00, 5.43053634e-02,\n",
      "        0.00000000e+00, 1.32426340e-02, 0.00000000e+00, 1.01607107e-02,\n",
      "        2.33296957e-02, 7.62477517e-03, 5.28246872e-02, 3.13927978e-02,\n",
      "        4.68221083e-02, 1.73802562e-02, 5.77410571e-02, 2.12943200e-02,\n",
      "        0.00000000e+00, 0.00000000e+00, 4.15032469e-02, 4.66578603e-02,\n",
      "        5.01768244e-03, 1.98969934e-02, 4.10840614e-03, 8.39858316e-03,\n",
      "        2.33639274e-02, 0.00000000e+00, 0.00000000e+00, 6.38308981e-03,\n",
      "        2.60149725e-02, 4.71629463e-02, 0.00000000e+00, 8.88706185e-03,\n",
      "        1.59179904e-02, 4.22774255e-02, 1.37361232e-04, 9.67059284e-04,\n",
      "        1.00240984e-03, 0.00000000e+00, 0.00000000e+00, 6.41528293e-02,\n",
      "        0.00000000e+00, 0.00000000e+00, 1.14045218e-02, 0.00000000e+00,\n",
      "        1.87705681e-02, 2.70475689e-02, 0.00000000e+00, 0.00000000e+00,\n",
      "        0.00000000e+00, 8.68791621e-03, 2.68446635e-02, 0.00000000e+00,\n",
      "        0.00000000e+00, 1.14780506e-02, 0.00000000e+00, 0.00000000e+00,\n",
      "        0.00000000e+00, 2.39940640e-02, 1.00949816e-02, 0.00000000e+00,\n",
      "        4.13028970e-02, 0.00000000e+00, 0.00000000e+00, 2.83154007e-02,\n",
      "        0.00000000e+00, 0.00000000e+00, 1.77522805e-02, 1.66376438e-02,\n",
      "        0.00000000e+00, 0.00000000e+00, 1.51478201e-02, 0.00000000e+00,\n",
      "        2.48664748e-02, 6.17086282e-03, 4.65702899e-02, 1.29600642e-02],\n",
      "       [1.64260548e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "        9.06638727e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "        1.78080276e-02, 2.42414661e-02, 1.76759046e-02, 1.70730576e-02,\n",
      "        0.00000000e+00, 0.00000000e+00, 1.90392360e-02, 0.00000000e+00,\n",
      "        2.79633328e-02, 0.00000000e+00, 3.30922604e-02, 0.00000000e+00,\n",
      "        3.76593322e-02, 0.00000000e+00, 1.24138058e-03, 0.00000000e+00,\n",
      "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.91455644e-02,\n",
      "        0.00000000e+00, 7.23853707e-03, 0.00000000e+00, 1.78815122e-03,\n",
      "        1.96473598e-02, 3.61207291e-03, 5.13847880e-02, 3.69598940e-02,\n",
      "        4.55737114e-02, 1.63950212e-02, 4.42113280e-02, 2.44849976e-02,\n",
      "        0.00000000e+00, 4.37015056e-04, 4.47224528e-02, 3.15608382e-02,\n",
      "        2.57811183e-03, 2.66750157e-02, 8.15930963e-03, 2.05779150e-02,\n",
      "        1.18456315e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "        2.45614778e-02, 4.71916460e-02, 0.00000000e+00, 2.07601395e-02,\n",
      "        5.30078076e-03, 3.61750275e-02, 0.00000000e+00, 1.75101438e-03,\n",
      "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 5.99427447e-02,\n",
      "        0.00000000e+00, 0.00000000e+00, 2.51417812e-02, 0.00000000e+00,\n",
      "        1.17122717e-02, 2.56426241e-02, 0.00000000e+00, 0.00000000e+00,\n",
      "        0.00000000e+00, 1.60839111e-02, 1.01526491e-02, 0.00000000e+00,\n",
      "        0.00000000e+00, 1.88365132e-02, 0.00000000e+00, 0.00000000e+00,\n",
      "        0.00000000e+00, 1.40433460e-02, 9.48114973e-03, 0.00000000e+00,\n",
      "        3.92781608e-02, 0.00000000e+00, 0.00000000e+00, 1.99006796e-02,\n",
      "        0.00000000e+00, 1.51627266e-03, 6.07033074e-03, 2.02361532e-02,\n",
      "        0.00000000e+00, 0.00000000e+00, 5.05761756e-03, 0.00000000e+00,\n",
      "        2.08298266e-02, 2.59572361e-03, 4.08432595e-02, 2.31552701e-02],\n",
      "       [3.61916833e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "        1.03749655e-01, 7.24605750e-04, 0.00000000e+00, 0.00000000e+00,\n",
      "        3.70004997e-02, 2.68608183e-02, 2.36241184e-02, 8.81871767e-03,\n",
      "        0.00000000e+00, 0.00000000e+00, 2.25565508e-02, 0.00000000e+00,\n",
      "        3.54701802e-02, 0.00000000e+00, 3.04226633e-02, 0.00000000e+00,\n",
      "        2.78190002e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "        4.81672352e-03, 0.00000000e+00, 0.00000000e+00, 5.61924614e-02,\n",
      "        0.00000000e+00, 1.61199048e-02, 0.00000000e+00, 1.39540499e-02,\n",
      "        2.59837583e-02, 7.90622458e-03, 4.93350811e-02, 2.96392031e-02,\n",
      "        4.93565053e-02, 1.64168067e-02, 6.18806481e-02, 1.91908535e-02,\n",
      "        0.00000000e+00, 0.00000000e+00, 3.98237407e-02, 5.04816920e-02,\n",
      "        6.16945419e-03, 1.61698814e-02, 2.16191635e-04, 4.73770173e-03,\n",
      "        2.70854160e-02, 0.00000000e+00, 0.00000000e+00, 1.05980095e-02,\n",
      "        2.56936997e-02, 4.80352864e-02, 0.00000000e+00, 3.76777351e-03,\n",
      "        2.10668445e-02, 4.43507768e-02, 1.22035062e-03, 0.00000000e+00,\n",
      "        2.34184787e-03, 0.00000000e+00, 0.00000000e+00, 6.54366016e-02,\n",
      "        0.00000000e+00, 0.00000000e+00, 3.93178686e-03, 0.00000000e+00,\n",
      "        2.08939724e-02, 2.93593686e-02, 0.00000000e+00, 0.00000000e+00,\n",
      "        0.00000000e+00, 6.26251847e-03, 3.38764898e-02, 0.00000000e+00,\n",
      "        0.00000000e+00, 1.04693174e-02, 0.00000000e+00, 0.00000000e+00,\n",
      "        0.00000000e+00, 2.51295120e-02, 1.37854535e-02, 0.00000000e+00,\n",
      "        4.32717651e-02, 0.00000000e+00, 0.00000000e+00, 2.99941078e-02,\n",
      "        0.00000000e+00, 0.00000000e+00, 2.10770275e-02, 1.36654759e-02,\n",
      "        0.00000000e+00, 0.00000000e+00, 1.88100561e-02, 0.00000000e+00,\n",
      "        2.71179713e-02, 1.00933239e-02, 4.75925356e-02, 1.11400913e-02],\n",
      "       [3.61916833e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "        1.03749655e-01, 7.24605750e-04, 0.00000000e+00, 0.00000000e+00,\n",
      "        3.70004997e-02, 2.68608183e-02, 2.36241184e-02, 8.81871767e-03,\n",
      "        0.00000000e+00, 0.00000000e+00, 2.25565508e-02, 0.00000000e+00,\n",
      "        3.54701802e-02, 0.00000000e+00, 3.04226633e-02, 0.00000000e+00,\n",
      "        2.78190002e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "        4.81672352e-03, 0.00000000e+00, 0.00000000e+00, 5.61924614e-02,\n",
      "        0.00000000e+00, 1.61199048e-02, 0.00000000e+00, 1.39540499e-02,\n",
      "        2.59837583e-02, 7.90622458e-03, 4.93350811e-02, 2.96392031e-02,\n",
      "        4.93565053e-02, 1.64168067e-02, 6.18806481e-02, 1.91908535e-02,\n",
      "        0.00000000e+00, 0.00000000e+00, 3.98237407e-02, 5.04816920e-02,\n",
      "        6.16945419e-03, 1.61698814e-02, 2.16191635e-04, 4.73770173e-03,\n",
      "        2.70854160e-02, 0.00000000e+00, 0.00000000e+00, 1.05980095e-02,\n",
      "        2.56936997e-02, 4.80352864e-02, 0.00000000e+00, 3.76777351e-03,\n",
      "        2.10668445e-02, 4.43507768e-02, 1.22035062e-03, 0.00000000e+00,\n",
      "        2.34184787e-03, 0.00000000e+00, 0.00000000e+00, 6.54366016e-02,\n",
      "        0.00000000e+00, 0.00000000e+00, 3.93178686e-03, 0.00000000e+00,\n",
      "        2.08939724e-02, 2.93593686e-02, 0.00000000e+00, 0.00000000e+00,\n",
      "        0.00000000e+00, 6.26251847e-03, 3.38764898e-02, 0.00000000e+00,\n",
      "        0.00000000e+00, 1.04693174e-02, 0.00000000e+00, 0.00000000e+00,\n",
      "        0.00000000e+00, 2.51295120e-02, 1.37854535e-02, 0.00000000e+00,\n",
      "        4.32717651e-02, 0.00000000e+00, 0.00000000e+00, 2.99941078e-02,\n",
      "        0.00000000e+00, 0.00000000e+00, 2.10770275e-02, 1.36654759e-02,\n",
      "        0.00000000e+00, 0.00000000e+00, 1.88100561e-02, 0.00000000e+00,\n",
      "        2.71179713e-02, 1.00933239e-02, 4.75925356e-02, 1.11400913e-02],\n",
      "       [3.61916833e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "        1.03749655e-01, 7.24605750e-04, 0.00000000e+00, 0.00000000e+00,\n",
      "        3.70004997e-02, 2.68608183e-02, 2.36241184e-02, 8.81871767e-03,\n",
      "        0.00000000e+00, 0.00000000e+00, 2.25565508e-02, 0.00000000e+00,\n",
      "        3.54701802e-02, 0.00000000e+00, 3.04226633e-02, 0.00000000e+00,\n",
      "        2.78190002e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "        4.81672352e-03, 0.00000000e+00, 0.00000000e+00, 5.61924614e-02,\n",
      "        0.00000000e+00, 1.61199048e-02, 0.00000000e+00, 1.39540499e-02,\n",
      "        2.59837583e-02, 7.90622458e-03, 4.93350811e-02, 2.96392031e-02,\n",
      "        4.93565053e-02, 1.64168067e-02, 6.18806481e-02, 1.91908535e-02,\n",
      "        0.00000000e+00, 0.00000000e+00, 3.98237407e-02, 5.04816920e-02,\n",
      "        6.16945419e-03, 1.61698814e-02, 2.16191635e-04, 4.73770173e-03,\n",
      "        2.70854160e-02, 0.00000000e+00, 0.00000000e+00, 1.05980095e-02,\n",
      "        2.56936997e-02, 4.80352864e-02, 0.00000000e+00, 3.76777351e-03,\n",
      "        2.10668445e-02, 4.43507768e-02, 1.22035062e-03, 0.00000000e+00,\n",
      "        2.34184787e-03, 0.00000000e+00, 0.00000000e+00, 6.54366016e-02,\n",
      "        0.00000000e+00, 0.00000000e+00, 3.93178686e-03, 0.00000000e+00,\n",
      "        2.08939724e-02, 2.93593686e-02, 0.00000000e+00, 0.00000000e+00,\n",
      "        0.00000000e+00, 6.26251847e-03, 3.38764898e-02, 0.00000000e+00,\n",
      "        0.00000000e+00, 1.04693174e-02, 0.00000000e+00, 0.00000000e+00,\n",
      "        0.00000000e+00, 2.51295120e-02, 1.37854535e-02, 0.00000000e+00,\n",
      "        4.32717651e-02, 0.00000000e+00, 0.00000000e+00, 2.99941078e-02,\n",
      "        0.00000000e+00, 0.00000000e+00, 2.10770275e-02, 1.36654759e-02,\n",
      "        0.00000000e+00, 0.00000000e+00, 1.88100561e-02, 0.00000000e+00,\n",
      "        2.71179713e-02, 1.00933239e-02, 4.75925356e-02, 1.11400913e-02],\n",
      "       [6.30993675e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "        8.32779408e-02, 0.00000000e+00, 8.10897327e-05, 0.00000000e+00,\n",
      "        7.54457572e-03, 2.24267989e-02, 1.48720909e-02, 2.20628437e-02,\n",
      "        0.00000000e+00, 0.00000000e+00, 1.72912367e-02, 0.00000000e+00,\n",
      "        2.49791779e-02, 0.00000000e+00, 3.53042223e-02, 0.00000000e+00,\n",
      "        4.32922691e-02, 0.00000000e+00, 2.68438179e-03, 0.00000000e+00,\n",
      "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.58108485e-02,\n",
      "        0.00000000e+00, 2.44029285e-03, 0.00000000e+00, 0.00000000e+00,\n",
      "        1.59090832e-02, 1.78218831e-03, 5.46750501e-02, 4.06685472e-02,\n",
      "        4.28149030e-02, 1.69771668e-02, 3.53331566e-02, 2.74532828e-02,\n",
      "        0.00000000e+00, 5.79679385e-04, 4.77935784e-02, 2.31059473e-02,\n",
      "        5.07636694e-04, 3.27066593e-02, 1.35594066e-02, 2.80594788e-02,\n",
      "        4.22124518e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "        2.44754348e-02, 4.65964116e-02, 0.00000000e+00, 3.00596431e-02,\n",
      "        0.00000000e+00, 3.20421830e-02, 0.00000000e+00, 6.03853073e-03,\n",
      "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 5.71410432e-02,\n",
      "        0.00000000e+00, 0.00000000e+00, 3.73025313e-02, 0.00000000e+00,\n",
      "        7.18657067e-03, 2.27995515e-02, 0.00000000e+00, 1.89972110e-03,\n",
      "        0.00000000e+00, 2.10641585e-02, 0.00000000e+00, 0.00000000e+00,\n",
      "        0.00000000e+00, 2.21701637e-02, 0.00000000e+00, 0.00000000e+00,\n",
      "        0.00000000e+00, 9.78709571e-03, 5.23827923e-03, 0.00000000e+00,\n",
      "        3.65940146e-02, 0.00000000e+00, 0.00000000e+00, 1.54686216e-02,\n",
      "        1.13911740e-03, 1.48893576e-02, 0.00000000e+00, 2.45285667e-02,\n",
      "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "        1.74011495e-02, 0.00000000e+00, 3.77052799e-02, 2.80703809e-02]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(6, 6), dtype=float32, numpy=\n",
      "array([[1., 1., 1., 1., 1., 0.],\n",
      "       [1., 1., 0., 0., 0., 1.],\n",
      "       [1., 0., 1., 0., 0., 0.],\n",
      "       [1., 0., 0., 1., 0., 0.],\n",
      "       [1., 0., 0., 0., 1., 0.],\n",
      "       [0., 1., 0., 0., 0., 1.]], dtype=float32)>)\n",
      "TensorShape([6, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.04472245]], dtype=float32)>"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes, adj = gen_smiles2graph('CO')\n",
    "model((nodes, adj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It outputs one number! That's always nice to have. Now we need to do some work to get a trainable dataset. Our dataset is a little bit complex because our features are tuples of tensors($\\mathbf{V}, \\mathbf{E}$) so that our dataset is a tuple of tuples: $\\left((\\mathbf{V}, \\mathbf{E}), y\\right)$. We use a **generator**, which is just a python function that can return multiple times. Our function returns once for every training example. Then we have to pass it to the `from_generator` dataset constructor which requires explicit declaration of the shapes of these examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example():\n",
    "    for i in range(len(soldata)):\n",
    "        graph = gen_smiles2graph(soldata.SMILES[i])        \n",
    "        sol = soldata.Solubility[i]\n",
    "        yield graph, sol\n",
    "data = tf.data.Dataset.from_generator(example, output_types=((tf.float32, tf.float32), tf.float32), \n",
    "                                      output_shapes=((tf.TensorShape([None, 100]), tf.TensorShape([None, None])), tf.TensorShape([])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whew, that's a lot. Now we can do our usual splitting of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = data.take(200)\n",
    "val_data = data.skip(200).take(200)\n",
    "train_data = data.skip(400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, time to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "[<tf.Tensor 'IteratorGetNext:0' shape=(None, 100) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, None) dtype=float32>]\n",
      "(<tf.Tensor 'functional_40/gcn_layer_88/Relu:0' shape=(None, 100) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, None) dtype=float32>)\n",
      "(<tf.Tensor 'functional_40/gcn_layer_89/Relu:0' shape=(None, 100) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, None) dtype=float32>)\n",
      "(<tf.Tensor 'functional_40/gcn_layer_90/Relu:0' shape=(None, 100) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, None) dtype=float32>)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /home/whitead/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /home/whitead/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /home/whitead/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/whitead/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/whitead/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/whitead/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\n        outputs = model.train_step(data)\n    /home/whitead/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:749 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /home/whitead/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/keras/engine/compile_utils.py:212 __call__\n        batch_dim = array_ops.shape(y_t)[0]\n    /home/whitead/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /home/whitead/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1024 _slice_helper\n        name=name)\n    /home/whitead/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /home/whitead/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1196 strided_slice\n        shrink_axis_mask=shrink_axis_mask)\n    /home/whitead/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py:10352 strided_slice\n        shrink_axis_mask=shrink_axis_mask, name=name)\n    /home/whitead/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:744 _apply_op_helper\n        attrs=attr_protos, op_def=op_def)\n    /home/whitead/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py:593 _create_op_internal\n        compute_device)\n    /home/whitead/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:3485 _create_op_internal\n        op_def=op_def)\n    /home/whitead/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1975 __init__\n        control_input_ops, op_def)\n    /home/whitead/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1815 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: slice index 0 of dimension 0 out of bounds. for '{{node strided_slice}} = StridedSlice[Index=DT_INT32, T=DT_INT32, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](Shape, strided_slice/stack, strided_slice/stack_1, strided_slice/stack_2)' with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = <0>, input[2] = <1>, input[3] = <1>.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-224-811111610524>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    821\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    696\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 697\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2855\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2856\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /home/whitead/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /home/whitead/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /home/whitead/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/whitead/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/whitead/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/whitead/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\n        outputs = model.train_step(data)\n    /home/whitead/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:749 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /home/whitead/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/keras/engine/compile_utils.py:212 __call__\n        batch_dim = array_ops.shape(y_t)[0]\n    /home/whitead/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /home/whitead/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1024 _slice_helper\n        name=name)\n    /home/whitead/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /home/whitead/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1196 strided_slice\n        shrink_axis_mask=shrink_axis_mask)\n    /home/whitead/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py:10352 strided_slice\n        shrink_axis_mask=shrink_axis_mask, name=name)\n    /home/whitead/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:744 _apply_op_helper\n        attrs=attr_protos, op_def=op_def)\n    /home/whitead/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py:593 _create_op_internal\n        compute_device)\n    /home/whitead/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:3485 _create_op_internal\n        op_def=op_def)\n    /home/whitead/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1975 __init__\n        control_input_ops, op_def)\n    /home/whitead/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1815 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: slice index 0 of dimension 0 out of bounds. for '{{node strided_slice}} = StridedSlice[Index=DT_INT32, T=DT_INT32, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](Shape, strided_slice/stack, strided_slice/stack_1, strided_slice/stack_2)' with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = <0>, input[2] = <1>, input[3] = <1>.\n"
     ]
    }
   ],
   "source": [
    "model.compile('adam', loss='mean_squared_error')\n",
    "result = model.fit(train_data, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message Passing Viewpoint\n",
    "\n",
    "One way to more broadly view a GCN layer is that it is a kind of \"message-passing\" layer.\n",
    "\n",
    "## Gated Graph Neural Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Battaglia General Equations\n",
    "\n",
    "In Battaglia et al {cite}`battaglia2018relational`, a general set of equations which captures nearly all GNNs was developed. They broke the GNN layer equations down into 3 update equations, which are like the GRU update in the GGN, and 3 aggregation equations, which are like summing over messages. There is one new idea here: graph feature vectors. A graph feature vector is a set of features which represent the whole graph or molecule. For example, when computing solubility it may have been useful to build-up a per-molecule feature vector that was eventually used to compute solubility. Any knid of per-molecule quantity like energy can be expressed as a graph-level feature vector. \n",
    "\n",
    "The first step in these equations is updating the edge feature vectors, written as $\\vec{e}_k$, which we haven't seen yet but is certainly possible:\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{e}^{'}_k = \\phi^e\\left( \\vec{e}_k, \\vec{v}_{rk}, \\vec{v}_{sk}, \\vec{u}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\vec{e}_k$ is the feature vector of edge $k$, $\\vec{v}_{rk}$ is the receiving node feature vector for edge $k$, $\\vec{v}_{sk}$ is the sending node feature vector for edge $k$, $\\vec{u}$ is the global graph feature vector, and $\\phi^e$ is one of the three update functions that the define the GNN layer. Note that these are meant to be general expressions and you define $\\phi^e$ for your specific GNN layer. The output edge updates are then aggregated with the first aggregation function:\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{e}^{'}_i = \\rho^{e\\rightarrow v}\\left( E_i^{'}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "where \\rho^{e\\rightarrow v} is our defined function and $E_i^{'}$ represents all edges in or out of node i. Having our aggregated edge, which is equivalent to our *message* previously, we can compute the node update:\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{v}^{'}_i = \\phi^v\\left( \\bar{e}^{'}_i, \\vec{v}_i, \\vec{u}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "This concludes the usual steps of a GNN layer. If you are updating global attributes or aggregating nodes or edges, the following additional steps may be defined:\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{e}^{'} = \\rho^{e\\rightarrow u}\\left( E^{'}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "This equation aggregates all messages across the whole graph. Then we can aggregate the new nodes across the whole graph:\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{v}^{'} = \\rho^{v\\rightarrow u}\\left( V^{'}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Then, we can compute the update to the global feature vector as:\n",
    "\\begin{equation}\n",
    "\\vec{u}^{'} = \\phi^u\\left( \\bar{e}^{'},\\bar{v}^{'}, \\vec{u}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformulating GNNs into Battaglia equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cited References\n",
    "\n",
    "```{bibliography} references.bib\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
